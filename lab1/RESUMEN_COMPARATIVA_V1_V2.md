# Resumen Comparativo: V1 vs V2 - Laboratorio 1 EXIST 2025

## üìä Resumen Ejecutivo

La segunda iteraci√≥n (V2) del proyecto logr√≥ **mejoras sustanciales** en los modelos principales mediante refinamiento del preprocesamiento y ajustes de hiperpar√°metros. El modelo √≥ptimo es **F2LLM-4B con texto original (tweet) V2**, alcanzando F1=0.8532, Recall=0.9185.

---

## üîÑ Cambios en Preprocesamiento V1 ‚Üí V2

### V1: Preprocesamiento B√°sico
- ‚úó Sin eliminaci√≥n de stopwords
- ‚úó Sin manejo de negaciones
- ‚úó Mantiene acentos espa√±oles (√°, √©, √±)
- ‚úó Sin normalizaci√≥n de elongaciones
- 13 features de ingenier√≠a

### V2: Preprocesamiento Mejorado
- ‚úÖ **Eliminaci√≥n de 313 stopwords** (NLTK espa√±ol)
- ‚úÖ **Preservaci√≥n cr√≠tica de negaciones**: {no, nunca, jam√°s, nada, nadie, tampoco, ni, sin}
- ‚úÖ **Marcado de contexto negativo**: Palabras tras negaciones ‚Üí NEG_palabra
- ‚úÖ **Normalizaci√≥n de acentos**: √°‚Üía, √©‚Üíe, √±‚Üín
- ‚úÖ **Normalizaci√≥n de elongaciones**: "siiii" ‚Üí "sii"
- ‚úÖ **Detecci√≥n de emojis mejorada**: Biblioteca `emoji` (m√°s precisa que regex)
- 16 features de ingenier√≠a (+3 nuevas: n_caps_words, n_elongations, n_negations)
- **Reducci√≥n de texto**: ~52% menos palabras en text_clean vs V1

### Impacto Clave
üî• **La preservaci√≥n de negaciones es CR√çTICA** para detecci√≥n de sexismo, ya que cambia completamente el significado sem√°ntico (ej: "no es inteligente" vs "es inteligente").

---

## üìà M√©tricas Comparativas: Mejores Modelos

### Modelos de Lenguaje (LLMs)

| Modelo | Texto | Versi√≥n | Accuracy | Precision | Recall | F1 | Œî F1 |
|--------|-------|---------|----------|-----------|--------|-----|------|
| **F2LLM-4B** | tweet | V1 | 0.8604 | 0.8294 | 0.8642 | 0.8464 | - |
| **F2LLM-4B** | tweet | V2 | **0.8593** | 0.7966 | **0.9185** | **0.8532** | **+0.0068** |
| F2LLM-4B | clean | V1 | 0.8473 | 0.8122 | 0.8543 | 0.8327 | - |
| F2LLM-4B | clean | V2 | 0.8341 | 0.7581 | 0.9210 | 0.8317 | -0.0010 |
| **KaLM** | tweet | V1 | **0.8363** | **0.8137** | **0.8198** | **0.8167** | - |
| **KaLM** | tweet | V2 | 0.8143 | 0.7682 | 0.8346 | 0.8000 | **-0.0167** |
| KaLM | clean | V1 | 0.8363 | 0.7963 | 0.8494 | 0.8220 | - |
| KaLM | clean | V2 | --- | --- | --- | --- | --- |
| **Ministral-3B** | ZS | V1 | **0.8264** | **0.7892** | 0.8321 | **0.8101** | - |
| **Ministral-3B** | ZS | V2 | 0.8143 | 0.7500 | **0.8741** | 0.8073 | -0.0028 |
| Ministral-3B | FT | V1 | 0.8451 | 0.8587 | 0.7802 | 0.8176 | - |
| Ministral-3B | FT | V2 | 0.5725 | 0.9000 | **0.0444** | **0.0847** | **-0.7329** |

**Notas:**
- üü¢ **F2LLM-4B tweet V2**: Mejor modelo general (+0.68 pp F1, +5.43 pp Recall)
- üî¥ **KaLM V2**: Empeora (-1.67 pp F1). Posible incompatibilidad con stopword removal
- ‚ö†Ô∏è **Ministral-3B FT V2**: **COLAPSO TOTAL** (-73.29 pp F1). Fine-tuning fall√≥ catastr√≥ficamente

---

### Modelos Cl√°sicos

| Modelo | Versi√≥n | Accuracy | Precision | Recall | F1 | Œî F1 |
|--------|---------|----------|-----------|--------|-----|------|
| **Stacking** | V1 | 0.7791 | 0.7898 | 0.6864 | 0.7345 | - |
| **Stacking** | V2 | **0.7824** | 0.7867 | **0.7012** | **0.7415** | **+0.0070** |
| **LogReg (TF-IDF)** | V1 | 0.7593 | 0.7657 | 0.6617 | 0.7099 | - |
| **LogReg (TF-IDF)** | V2 | **0.7725** | **0.7845** | **0.6741** | **0.7251** | **+0.0152** |

**Impacto V2 en cl√°sicos:**
- ‚úÖ Mejoras consistentes (+0.70 a +1.52 pp F1)
- ‚úÖ Eliminaci√≥n de 313 stopwords reduce ruido y mejora vectorizaci√≥n TF-IDF
- ‚úÖ Reducci√≥n de vocabulario ~52% mejora generalizaci√≥n

---

## üéØ Tweet Original vs Text Clean

### ¬øCu√°l funciona mejor?

| Modelo | V1: Tweet F1 | V1: Clean F1 | V2: Tweet F1 | V2: Clean F1 | Mejor |
|--------|--------------|--------------|--------------|--------------|-------|
| F2LLM-4B | **0.8464** | 0.8327 | **0.8532** | 0.8317 | **Tweet** |
| KaLM | 0.8167 | **0.8220** | **0.8000** | --- | Clean (V1) |

### Conclusi√≥n: Depende del Tipo de Modelo

#### ‚úÖ **Para LLMs (Transformers)**: Usar **texto original (tweet)**
- Los transformers pre-entrenados ya manejan bien el ruido
- Se benefician del contexto completo (URLs, emojis, menciones)
- F2LLM-4B: Tweet (0.8532) > Clean (0.8317), Œî=-2.15 pp

#### ‚úÖ **Para modelos cl√°sicos (TF-IDF)**: Usar **text_clean V2**
- La eliminaci√≥n de stopwords reduce vocabulario y mejora vectorizaci√≥n
- LogReg V2: +1.52 pp F1 vs V1 gracias al preprocesamiento

---

## üîù Ranking Final: Mejores Modelos V2

| Rank | Modelo | Accuracy | Precision | Recall | F1 |
|------|--------|----------|-----------|--------|-----|
| ü•á | **F2LLM-4B (tweet)** | 0.8593 | 0.7966 | **0.9185** | **0.8532** |
| ü•à | F2LLM-4B (clean) | 0.8341 | 0.7581 | **0.9210** | 0.8317 |
| ü•â | Ministral-3B (ZS) | 0.8143 | 0.7500 | 0.8741 | 0.8073 |
| 4 | KaLM (tweet) | 0.8143 | 0.7682 | 0.8346 | 0.8000 |
| 5 | Stacking (TF-IDF) | 0.7824 | 0.7867 | 0.7012 | 0.7415 |
| 6 | LogReg (TF-IDF) | 0.7725 | 0.7845 | 0.6741 | 0.7251 |

**Brecha LLM vs Cl√°sicos**: +10.93 pp F1 (F2LLM-4B vs Stacking)

---

## üé≤ Ensemble Top 5: ¬øAporta Mejora?

### Configuraci√≥n Ensemble V2
- **M√©todo**: Votaci√≥n mayoritaria (simple majority voting)
- **Modelos**: F2LLM-4B (tweet + clean), KaLM (tweet), Ministral-3B (ZS), LogReg (TF-IDF)

### Resultado

| M√©trica | F2LLM-4B Individual | Ensemble Top 5 | Diferencia |
|---------|---------------------|----------------|------------|
| Accuracy | 0.8593 | 0.8593 | 0.0000 |
| Precision | 0.7966 | 0.7966 | 0.0000 |
| Recall | 0.9185 | 0.9185 | 0.0000 |
| F1 | 0.8532 | 0.8532 | **0.0000** |

### ‚ùå Conclusi√≥n: **Ensemble NO mejora**

**¬øPor qu√©?**
1. F2LLM-4B domina el ensemble (90%+ votos coincidentes)
2. Los dem√°s modelos no aportan diversidad suficiente
3. La votaci√≥n mayoritaria no corrige errores del mejor modelo

**Recomendaci√≥n**: Usar **F2LLM-4B individual** (simplicidad, eficiencia, mismo resultado)

---

## ‚ö†Ô∏è Casos Problem√°ticos

### 1. KaLM Empeora en V2 (-1.67 pp F1)

**Hip√≥tesis:**
- Modelo pre-entrenado con texto menos procesado
- Eliminaci√≥n agresiva de stopwords elimina patrones importantes
- Requiere ajustes de hiperpar√°metros espec√≠ficos para V2

**Acci√≥n futura**: Probar fine-tuning con learning rate m√°s bajo

---

### 2. Ministral-3B FT: Colapso Catastr√≥fico

**M√©tricas V2:**
- F1: 0.0847 (vs 0.8176 en V1) ‚Üí **-73.29 pp**
- Recall: 0.0444 ‚Üí **Casi no detecta clase positiva**

**Hip√≥tesis del fallo:**
1. Learning rate inadecuado (demasiado alto/bajo)
2. Datos de entrenamiento corruptos
3. Incompatibilidad LoRA + cuantizaci√≥n FP8
4. Warmup steps insuficientes

**Acci√≥n urgente**: Investigar logs de entrenamiento, probar LR={1e-5, 5e-5, 1e-4}

---

## üìä Cambios Cuantitativos V1 ‚Üí V2

| Modelo (Tweet) | Œî Acc | Œî Prec | Œî Rec | Œî F1 | Evaluaci√≥n |
|----------------|-------|--------|-------|------|------------|
| F2LLM-4B | -0.0011 | -0.0328 | **+0.0543** | **+0.0068** | ‚úÖ Mejora |
| KaLM | -0.0220 | -0.0455 | +0.0148 | **-0.0167** | ‚ùå Empeora |
| Ministral-3B (ZS) | -0.0121 | -0.0392 | **+0.0420** | -0.0028 | ‚âà Similar |
| Stacking | +0.0033 | -0.0031 | **+0.0148** | **+0.0070** | ‚úÖ Mejora |
| LogReg | **+0.0132** | +0.0188 | +0.0124 | **+0.0152** | ‚úÖ Mejora |

**Patrones:**
- ‚úÖ F2LLM-4B y modelos cl√°sicos: **Mejoran** con V2
- ‚öñÔ∏è Trade-off com√∫n: **Recall aumenta**, **Precision disminuye**
- ‚ùå KaLM: **Empeora** en todas las m√©tricas (requiere investigaci√≥n)

---

## üéì Lecciones Aprendidas

### 1. Preservaci√≥n de Negaciones es CR√çTICA
- Mejora recall en F2LLM-4B: +5.43 pp
- Esencial para detecci√≥n de sexismo (negaci√≥n cambia significado)
- **Recomendaci√≥n**: Siempre preservar negaciones en NLP para espa√±ol

### 2. Texto Original > Preprocesado para LLMs
- Transformers aprovechan contexto completo
- Preprocesamiento elimina informaci√≥n valiosa (URLs, emojis, menciones)
- **Recomendaci√≥n**: Minimal preprocessing para modelos transformer

### 3. Preprocesamiento Beneficia Modelos Cl√°sicos
- Eliminaci√≥n de stopwords mejora TF-IDF (+1.52 pp LogReg)
- Reducci√≥n de vocabulario mejora generalizaci√≥n
- **Recomendaci√≥n**: Aggressive cleaning para ML tradicional

### 4. Ensemble No Siempre Mejora
- Si el mejor modelo domina, votaci√≥n no aporta
- Necesitas diversidad en arquitecturas/estrategias
- **Recomendaci√≥n**: Evaluar ensemble en validaci√≥n antes de usar en test

### 5. Fine-tuning con LoRA Requiere Supervisi√≥n
- Ministral-3B FT colaps√≥ (F1: 0.0847)
- Monitorear m√©tricas durante entrenamiento
- **Recomendaci√≥n**: Early stopping, learning rate scheduling, checkpoints frecuentes

---

## üèÜ Recomendaci√≥n Final para EXIST 2025

### Modelo Seleccionado: **F2LLM-4B (tweet) V2**

**Justificaci√≥n:**
- ‚úÖ **Mejor F1 general**: 0.8532
- ‚úÖ **Excelente Recall**: 0.9185 (detecta 91.85% de casos sexistas)
- ‚úÖ **Accuracy competitiva**: 0.8593
- ‚úÖ **Ensemble no aporta mejora**: Simplicidad sin sacrificar rendimiento
- ‚úÖ **Balance √≥ptimo**: Precision-Recall ajustado para detecci√≥n de sexismo

**Archivo de predicci√≥n:**
```
results_v2/F2LLM-4B/predictions/BeingChillingWeWillWin_f2llm4B.json
```

---

## üìù Trabajo Futuro Prioritario

### Alta Prioridad
1. **Investigar colapso de Ministral-3B FT** (debugging urgente)
2. **An√°lisis de errores cualitativo** (identificar patrones de fallo)
3. **Ensemble avanzado** (stacking con meta-learner)
4. **Optimizaci√≥n de hiperpar√°metros** (Optuna/Ray Tune)

### Media Prioridad
5. Evaluar modelos m√°s recientes (Llama 3, Mixtral, RoBERTa-es)
6. Augmentaci√≥n de datos (back-translation, parafraseo)
7. An√°lisis de sesgo y equidad
8. Explicabilidad (LIME/SHAP, attention maps)

---

## üìå Conclusi√≥n

La iteraci√≥n V2 demuestra el valor del refinamiento iterativo:
- ‚úÖ **Mejoras significativas** en F2LLM-4B y modelos cl√°sicos
- ‚úÖ **Preservaci√≥n de negaciones** mejor√≥ recall +5.43 pp
- ‚ö†Ô∏è **Algunos modelos empeoran** (KaLM, Ministral-3B FT)
- üìà **Brecha LLM-Cl√°sicos**: +10.93 pp F1 (justifica uso de transformers)

**El modelo F2LLM-4B (tweet) V2 es la mejor soluci√≥n** para competici√≥n EXIST 2025, alcanzando F1=0.8532 con recall excepcional (0.9185).

---

**Generado**: 2026-02-27  
**Proyecto**: Lab 1 EXIST 2025 - Detecci√≥n de Sexismo  
**Autores**: Shiyi Cheng - Pablo Segovia Mart√≠nez
