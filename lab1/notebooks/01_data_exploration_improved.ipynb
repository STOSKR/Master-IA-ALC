{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e2beac",
   "metadata": {},
   "source": [
    "# Exploración de Datos - Versión Mejorada\n",
    "\n",
    "Análisis conciso y accionable del dataset EXIST2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbef13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "with open('../lab1_materials/dataset_task1_exist2025/training.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(list(train_data.values()))\n",
    "df['task1'] = df['labels_task1'].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "print(f\"Muestras: {len(df)}\\nClases: {df['task1'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c1a57",
   "metadata": {},
   "source": [
    "## Análisis de Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb29dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de vocabulario por clase\n",
    "def analyze_vocabulary(df, label_col='task1'):\n",
    "    results = {}\n",
    "    for label in df[label_col].unique():\n",
    "        texts = df[df[label_col] == label]['tweet'].tolist()\n",
    "        all_words = ' '.join(texts).lower().split()\n",
    "        vocab = set(all_words)\n",
    "        results[label] = {\n",
    "            'vocab_size': len(vocab),\n",
    "            'total_words': len(all_words),\n",
    "            'unique_ratio': len(vocab) / len(all_words)\n",
    "        }\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "vocab_stats = analyze_vocabulary(df)\n",
    "print(\"Estadísticas de vocabulario por clase:\")\n",
    "print(vocab_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd2f0b",
   "metadata": {},
   "source": [
    "## Palabras Discriminativas (Chi-square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ccfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras más discriminativas usando chi-square\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=5, lowercase=True)\n",
    "X = vectorizer.fit_transform(df['tweet'])\n",
    "y = (df['task1'] == 'YES').astype(int)\n",
    "\n",
    "chi2_scores = []\n",
    "for i in range(X.shape[1]):\n",
    "    contingency = pd.crosstab(X[:, i].toarray().ravel(), y)\n",
    "    chi2, p_value, _, _ = chi2_contingency(contingency)\n",
    "    chi2_scores.append((vectorizer.get_feature_names_out()[i], chi2, p_value))\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_scores, columns=['word', 'chi2', 'p_value']).sort_values('chi2', ascending=False)\n",
    "print(\"\\nTop 20 palabras discriminativas:\")\n",
    "print(chi2_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24bcdda",
   "metadata": {},
   "source": [
    "## N-gramas Discriminativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60300d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de bigramas por clase\n",
    "def get_top_ngrams(texts, n=2, top_k=15):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), max_features=500, min_df=3)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    counts = X.sum(axis=0).A1\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    return sorted(zip(ngrams, counts), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "for label in ['YES', 'NO']:\n",
    "    texts = df[df['task1'] == label]['tweet'].tolist()\n",
    "    bigrams = get_top_ngrams(texts, n=2, top_k=10)\n",
    "    print(f\"\\nTop bigramas en clase {label}:\")\n",
    "    for ngram, count in bigrams:\n",
    "        print(f\"  {ngram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfbbd3",
   "metadata": {},
   "source": [
    "## Análisis de Patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de patrones textuales\n",
    "patterns = {\n",
    "    'urls': r'http[s]?://\\S+',\n",
    "    'mentions': r'@\\w+',\n",
    "    'hashtags': r'#\\w+',\n",
    "    'exclamations': r'!+',\n",
    "    'questions': r'\\?+',\n",
    "    'caps_words': r'\\b[A-Z]{2,}\\b'\n",
    "}\n",
    "\n",
    "for pattern_name, pattern in patterns.items():\n",
    "    df[f'has_{pattern_name}'] = df['tweet'].str.contains(pattern, regex=True)\n",
    "\n",
    "# Correlación de patrones con etiqueta\n",
    "pattern_cols = [f'has_{p}' for p in patterns.keys()]\n",
    "df['label_binary'] = (df['task1'] == 'YES').astype(int)\n",
    "\n",
    "print(\"\\nCorrelación de patrones con sexismo:\")\n",
    "for col in pattern_cols:\n",
    "    corr = df[[col, 'label_binary']].corr().iloc[0, 1]\n",
    "    print(f\"  {col}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8839ce6",
   "metadata": {},
   "source": [
    "## Análisis de Longitud y Complejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features de complejidad\n",
    "df['word_count'] = df['tweet'].str.split().str.len()\n",
    "df['char_count'] = df['tweet'].str.len()\n",
    "df['avg_word_len'] = df['char_count'] / df['word_count']\n",
    "df['caps_ratio'] = df['tweet'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# Comparar por clase\n",
    "complexity_features = ['word_count', 'char_count', 'avg_word_len', 'caps_ratio']\n",
    "print(\"\\nEstadísticas por clase:\")\n",
    "print(df.groupby('task1')[complexity_features].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e8dbc",
   "metadata": {},
   "source": [
    "## Análisis de Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement vs dificultad\n",
    "df['agreement'] = df['labels_task1'].apply(lambda x: Counter(x).most_common(1)[0][1] / len(x))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribución de agreement\n",
    "df['agreement'].hist(bins=20, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title('Distribución de Agreement')\n",
    "axes[0].set_xlabel('Agreement')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Agreement por clase\n",
    "df.boxplot(column='agreement', by='task1', ax=axes[1])\n",
    "axes[1].set_title('Agreement por Clase')\n",
    "axes[1].set_xlabel('Clase')\n",
    "axes[1].set_ylabel('Agreement')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAgreement promedio: {df['agreement'].mean():.3f}\")\n",
    "print(f\"Muestras con bajo agreement (<0.6): {(df['agreement'] < 0.6).sum()} ({(df['agreement'] < 0.6).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aad3ce",
   "metadata": {},
   "source": [
    "## Resumen de Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33720cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"INSIGHTS CLAVE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. Desbalance de clases: {df['task1'].value_counts()['YES']/len(df)*100:.1f}% positivos\")\n",
    "print(f\"2. Agreement promedio: {df['agreement'].mean():.2f}\")\n",
    "print(f\"3. Vocabulario único YES/NO: {vocab_stats.loc['YES', 'vocab_size']}/{vocab_stats.loc['NO', 'vocab_size']}\")\n",
    "print(f\"4. Palabras más discriminativas: ver chi-square\")\n",
    "print(f\"5. Patrones importantes: URLs, mentions, exclamations\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
