{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f235c94",
   "metadata": {},
   "source": [
    "# Ensemble de Modelos\n",
    "Combina las predicciones de los mejores modelos mediante votación mayoritaria y promedio de probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acda8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbf9c0",
   "metadata": {},
   "source": [
    "## 1. Cargar Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9059b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../preprocessed_data/val_preprocessed_v2.json', 'r', encoding='utf-8') as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "gold_dict = {item['id_EXIST']: item['task1'] for item in gold_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "620a12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Ministral8B_ft': '../results/BeingChillingWeWillWin_3Ministral8B_ft.json',\n",
    "    'Ministral8B': '../results/BeingChillingWeWillWin_3Ministral8B.json',\n",
    "    'f2llm4B': '../results/BeingChillingWeWillWin_f2llm4B.json',\n",
    "    'f2llm4B_clean': '../results/BeingChillingWeWillWin_f2llm4Bclean.json',\n",
    "    'KaLM': '../results/BeingChillingWeWillWin_KaLM.json',\n",
    "    'KaLM_clean': '../results/BeingChillingWeWillWin_KaLMclean.json'\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "for name, path in models.items():\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            predictions[name] = {item['id']: item['value'] for item in data}\n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"Warning: Could not load {name} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bec09",
   "metadata": {},
   "source": [
    "## 2. Evaluar Modelos Individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46c854a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No matching IDs between predictions and gold standard (test vs validation set)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, preds in predictions.items():\n",
    "    common_ids = set(preds.keys()) & set(gold_dict.keys())\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        continue\n",
    "    \n",
    "    y_true = [gold_dict[id_] for id_ in common_ids]\n",
    "    y_pred = [preds[id_] for id_ in common_ids]\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Samples': len(common_ids),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, pos_label='YES'),\n",
    "        'Recall': recall_score(y_true, y_pred, pos_label='YES'),\n",
    "        'F1': f1_score(y_true, y_pred, pos_label='YES')\n",
    "    })\n",
    "\n",
    "if len(results) > 0:\n",
    "    df_results = pd.DataFrame(results).sort_values('F1', ascending=False)\n",
    "    print(\"Individual Model Results:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "else:\n",
    "    df_results = pd.DataFrame()\n",
    "    print(\"Warning: No matching IDs between predictions and gold standard (test vs validation set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88972ca5",
   "metadata": {},
   "source": [
    "## 3. Ensemble por Votación Mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6115309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_ensemble(predictions_dict, ids):\n",
    "    ensemble_preds = {}\n",
    "    for id_ in ids:\n",
    "        votes = [preds[id_] for preds in predictions_dict.values() if id_ in preds]\n",
    "        if votes:\n",
    "            ensemble_preds[id_] = Counter(votes).most_common(1)[0][0]\n",
    "    return ensemble_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30d7d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_configs = [\n",
    "    {'name': 'All_Models', 'models': list(predictions.keys())},\n",
    "    {'name': 'Top_3_Best_F1', 'models': df_results.head(3)['Model'].tolist() if len(results) >= 3 and len(df_results) > 0 else list(predictions.keys())[:3]},\n",
    "    {'name': 'Ministral_Only', 'models': [k for k in predictions.keys() if 'Ministral' in k or '3M' in k]},\n",
    "    {'name': 'Fine_tuned_Models', 'models': [k for k in predictions.keys() if 'ft' in k or 'clean' in k]}\n",
    "]\n",
    "\n",
    "ensemble_results = []\n",
    "\n",
    "for config in ensemble_configs:\n",
    "    if not config['models']:\n",
    "        continue\n",
    "    \n",
    "    filtered_preds = {k: v for k, v in predictions.items() if k in config['models']}\n",
    "    common_ids = set(filtered_preds[list(filtered_preds.keys())[0]].keys())\n",
    "    for preds in filtered_preds.values():\n",
    "        common_ids &= set(preds.keys())\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        continue\n",
    "    \n",
    "    ensemble_preds = voting_ensemble(filtered_preds, common_ids)\n",
    "    gold_ids = set(gold_dict.keys()) & common_ids\n",
    "    \n",
    "    if len(gold_ids) == 0:\n",
    "        ensemble_results.append({\n",
    "            'Ensemble': config['name'],\n",
    "            'N_Models': len(config['models']),\n",
    "            'Samples': len(common_ids),\n",
    "            'Accuracy': None,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1': None\n",
    "        })\n",
    "    else:\n",
    "        y_true = [gold_dict[id_] for id_ in gold_ids]\n",
    "        y_pred = [ensemble_preds[id_] for id_ in gold_ids]\n",
    "        \n",
    "        ensemble_results.append({\n",
    "            'Ensemble': config['name'],\n",
    "            'N_Models': len(config['models']),\n",
    "            'Samples': len(gold_ids),\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred, pos_label='YES'),\n",
    "            'Recall': recall_score(y_true, y_pred, pos_label='YES'),\n",
    "            'F1': f1_score(y_true, y_pred, pos_label='YES')\n",
    "        })\n",
    "\n",
    "df_ensemble = pd.DataFrame(ensemble_results)\n",
    "if not df_ensemble.empty and df_ensemble['F1'].notna().any():\n",
    "    df_ensemble = df_ensemble.sort_values('F1', ascending=False)\n",
    "    print(\"Ensemble Results (Majority Voting):\")\n",
    "    print(df_ensemble.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504751ed",
   "metadata": {},
   "source": [
    "## 4. Comparación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b53550c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid results with F1 scores\n"
     ]
    }
   ],
   "source": [
    "if len(df_results) > 0 or len(df_ensemble) > 0:\n",
    "    dfs_to_concat = []\n",
    "    if len(df_results) > 0:\n",
    "        dfs_to_concat.append(df_results.assign(Type='Individual'))\n",
    "    if len(df_ensemble) > 0:\n",
    "        dfs_to_concat.append(df_ensemble.rename(columns={'Ensemble': 'Model', 'N_Models': 'Samples'}).assign(Type='Ensemble'))\n",
    "    \n",
    "    all_results = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "    \n",
    "    # Convert numeric columns to float and filter out rows with null F1\n",
    "    numeric_cols = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    for col in numeric_cols:\n",
    "        if col in all_results.columns:\n",
    "            all_results[col] = pd.to_numeric(all_results[col], errors='coerce')\n",
    "    \n",
    "    all_results = all_results.dropna(subset=['F1'])\n",
    "    \n",
    "    if len(all_results) > 0:\n",
    "        print(\"Top 10 Models (Individual + Ensemble):\")\n",
    "        print(all_results.nlargest(10, 'F1')[['Type', 'Model', 'F1', 'Accuracy', 'Precision', 'Recall']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"No valid results with F1 scores\")\n",
    "else:\n",
    "    print(\"No results available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5664227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results available for visualization\n"
     ]
    }
   ],
   "source": [
    "if 'all_results' in dir() and len(all_results) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    data_to_plot = all_results.nlargest(10, 'F1')\n",
    "    x = range(len(data_to_plot))\n",
    "\n",
    "    plt.bar(x, data_to_plot['F1'], alpha=0.7, color=['green' if t == 'Ensemble' else 'blue' for t in data_to_plot['Type']])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score Comparison: Individual vs Ensemble')\n",
    "    plt.xticks(x, data_to_plot['Model'], rotation=45, ha='right')\n",
    "    plt.legend(['Ensemble', 'Individual'])\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c094f15",
   "metadata": {},
   "source": [
    "## 5. Matriz de Confusión del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f1903b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common IDs found for confusion matrix\n"
     ]
    }
   ],
   "source": [
    "if len(df_ensemble) > 0 and df_ensemble['F1'].notna().any():\n",
    "    best_config = ensemble_configs[df_ensemble['F1'].idxmax()]\n",
    "else:\n",
    "    best_config = ensemble_configs[0] if len(ensemble_configs) > 0 else None\n",
    "\n",
    "if best_config is None:\n",
    "    print(\"No ensemble configuration available\")\n",
    "else:\n",
    "    filtered_preds = {k: v for k, v in predictions.items() if k in best_config['models']}\n",
    "    common_ids = set(gold_dict.keys())\n",
    "    for preds in filtered_preds.values():\n",
    "        common_ids &= set(preds.keys())\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(\"No common IDs found for confusion matrix\")\n",
    "    else:\n",
    "        ensemble_preds = voting_ensemble(filtered_preds, common_ids)\n",
    "        y_true = [gold_dict[id_] for id_ in common_ids]\n",
    "        y_pred = [ensemble_preds[id_] for id_ in common_ids]\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred, labels=['NO', 'YES'])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NO', 'YES'], yticklabels=['NO', 'YES'])\n",
    "        plt.title(f'Confusion Matrix: {best_config[\"name\"]}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, labels=['NO', 'YES']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558894e5",
   "metadata": {},
   "source": [
    "## 6. Análisis de Acuerdo entre Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e13a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for agreement analysis\n"
     ]
    }
   ],
   "source": [
    "if 'common_ids' in dir() and 'filtered_preds' in dir() and 'ensemble_preds' in dir() and len(common_ids) > 0:\n",
    "    agreement_analysis = []\n",
    "\n",
    "    for id_ in common_ids:\n",
    "        votes = [preds[id_] for preds in filtered_preds.values()]\n",
    "        yes_votes = votes.count('YES')\n",
    "        total_votes = len(votes)\n",
    "        \n",
    "        agreement_analysis.append({\n",
    "            'id': id_,\n",
    "            'true': gold_dict[id_],\n",
    "            'ensemble': ensemble_preds[id_],\n",
    "            'yes_votes': yes_votes,\n",
    "            'total_votes': total_votes,\n",
    "            'agreement': 'unanimous' if yes_votes in [0, total_votes] else 'split'\n",
    "        })\n",
    "\n",
    "    df_agreement = pd.DataFrame(agreement_analysis)\n",
    "\n",
    "    print(\"\\nAgreement Analysis:\")\n",
    "    print(f\"Unanimous cases: {(df_agreement['agreement'] == 'unanimous').sum()} ({(df_agreement['agreement'] == 'unanimous').mean()*100:.1f}%)\")\n",
    "    print(f\"Split cases: {(df_agreement['agreement'] == 'split').sum()} ({(df_agreement['agreement'] == 'split').mean()*100:.1f}%)\")\n",
    "\n",
    "    unanimous_correct = ((df_agreement['agreement'] == 'unanimous') & (df_agreement['true'] == df_agreement['ensemble'])).sum()\n",
    "    split_correct = ((df_agreement['agreement'] == 'split') & (df_agreement['true'] == df_agreement['ensemble'])).sum()\n",
    "\n",
    "    print(f\"\\nAccuracy in unanimous cases: {unanimous_correct}/{(df_agreement['agreement'] == 'unanimous').sum()}\")\n",
    "    print(f\"Accuracy in split cases: {split_correct}/{(df_agreement['agreement'] == 'split').sum()}\")\n",
    "else:\n",
    "    print(\"No data available for agreement analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a18293",
   "metadata": {},
   "source": [
    "## 7. Guardar Mejor Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7e1c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../results/BeingChillingWeWillWin_ensemble_All_Models.json\n",
      "Models: Ministral8B_ft, Ministral8B, f2llm4B, f2llm4B_clean, KaLM, KaLM_clean\n",
      "Predictions: 934\n"
     ]
    }
   ],
   "source": [
    "if len(ensemble_configs) > 0:\n",
    "    if len(df_ensemble) > 0 and df_ensemble['F1'].notna().any():\n",
    "        best_idx = df_ensemble['F1'].idxmax()\n",
    "        best_config = ensemble_configs[best_idx]\n",
    "    else:\n",
    "        best_config = ensemble_configs[0]\n",
    "    \n",
    "    filtered_preds = {k: v for k, v in predictions.items() if k in best_config['models']}\n",
    "    common_ids = set(filtered_preds[list(filtered_preds.keys())[0]].keys())\n",
    "    for preds in filtered_preds.values():\n",
    "        common_ids &= set(preds.keys())\n",
    "    \n",
    "    if len(common_ids) > 0:\n",
    "        ensemble_preds = voting_ensemble(filtered_preds, common_ids)\n",
    "        output_data = [{'test_case': 'EXIST2025', 'id': id_, 'value': pred} for id_, pred in ensemble_preds.items()]\n",
    "        output_path = f'../results/BeingChillingWeWillWin_ensemble_{best_config[\"name\"]}.json'\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved: {output_path}\")\n",
    "        print(f\"Models: {', '.join(best_config['models'])}\")\n",
    "        print(f\"Predictions: {len(ensemble_preds)}\")\n",
    "    else:\n",
    "        print(\"No common IDs to save\")\n",
    "else:\n",
    "    print(\"No ensemble configurations available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a669c",
   "metadata": {},
   "source": [
    "## 8. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69f90150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot generate summary: missing individual or ensemble results\n"
     ]
    }
   ],
   "source": [
    "best_individual = df_results.iloc[0] if len(df_results) > 0 else None\n",
    "best_ensemble = df_ensemble.iloc[0] if len(df_ensemble) > 0 else None\n",
    "\n",
    "if best_individual is not None and best_ensemble is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest Individual: {best_individual['Model']}\")\n",
    "    print(f\"F1: {best_individual['F1']:.4f} | Accuracy: {best_individual['Accuracy']:.4f}\")\n",
    "    print(f\"\\nBest Ensemble: {best_ensemble['Ensemble']}\")\n",
    "    print(f\"F1: {best_ensemble['F1']:.4f} | Accuracy: {best_ensemble['Accuracy']:.4f}\")\n",
    "    improvement = best_ensemble['F1'] - best_individual['F1']\n",
    "    improvement_pct = (improvement / best_individual['F1'] * 100)\n",
    "    print(f\"\\nImprovement: {improvement:.4f} (+{improvement_pct:.2f}%)\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"Cannot generate summary: missing individual or ensemble results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
