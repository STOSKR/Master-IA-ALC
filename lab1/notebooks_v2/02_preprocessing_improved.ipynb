{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2758c7",
   "metadata": {},
   "source": [
    "# Preprocesamiento Mejorado\n",
    "\n",
    "Mejoras:\n",
    "- **Stopwords con NLTK**: 313 stopwords en español (vs ~60 manuales)\n",
    "- **Preservación de negaciones**: CRÍTICO para detección de sexismo\n",
    "- **Detección de emojis con librería `emoji`**: Más completa y actualizada\n",
    "- Normalización de elongaciones\n",
    "- N-gramas de caracteres\n",
    "- Mejor manejo de negaciones con marcado de contexto\n",
    "\n",
    "## Librerías especializadas usadas:\n",
    "- `nltk.corpus.stopwords`: Stopwords completas por idioma\n",
    "- `emoji`: Detección y manejo de emojis Unicode actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a62906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /home/alumno.upv.es/scheng1/.conda/envs/RFA2526pt/lib/python3.12/site-packages (2.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1cd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librerías para stopwords y emojis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "# Seeds para reproducibilidad\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bd060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando stopwords...\n"
     ]
    }
   ],
   "source": [
    "# Descargar stopwords de NLTK (solo necesario la primera vez)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Descargando stopwords...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Obtener stopwords de español\n",
    "NLTK_STOPWORDS = set(stopwords.words('spanish'))\n",
    "\n",
    "# Negaciones CRÍTICAS para análisis de sentimiento/sexismo\n",
    "# NO se deben eliminar porque cambian completamente el significado\n",
    "NEGATIONS = {'no', 'nunca', 'jamás', 'nada', 'nadie', 'tampoco', 'ni', 'sin'}\n",
    "\n",
    "# Filtrar negaciones de las stopwords\n",
    "SPANISH_STOPWORDS = NLTK_STOPWORDS - NEGATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f449a0",
   "metadata": {},
   "source": [
    "## Cargar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c58991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6064, Test: 934\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('..') / 'lab1_materials' / 'dataset_task1_exist2025'\n",
    "\n",
    "with open(data_path / 'training.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(data_path / 'test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(list(train_data.values()))\n",
    "test_df = pd.DataFrame(list(test_data.values()))\n",
    "\n",
    "# Etiquetas con majority vote\n",
    "train_df['task1'] = train_df['labels_task1'].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "train_df['task2'] = train_df['labels_task2'].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "train_df['task3'] = train_df['labels_task3'].apply(lambda x: str(Counter([str(i) for i in x]).most_common(1)[0][0]))\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fc416",
   "metadata": {},
   "source": [
    "## Funciones de Preprocesamiento Mejoradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d07f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patrones de regex para detección\n",
    "URL_PATTERN = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "MENTION_PATTERN = re.compile(r'@\\w+')\n",
    "HASHTAG_PATTERN = re.compile(r'#(\\w+)')\n",
    "ELONGATION_PATTERN = re.compile(r'(.)\\1{2,}')  # Detecta repeticiones: siii -> si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab1a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_elongations(text):\n",
    "    \"\"\"Normaliza elongaciones: siiiii -> sii\"\"\"\n",
    "    return ELONGATION_PATTERN.sub(r'\\1\\1', text)\n",
    "\n",
    "def remove_accents(text):\n",
    "    \"\"\"Remueve acentos españoles\"\"\"\n",
    "    replacements = {\n",
    "        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "        'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U',\n",
    "        'ñ': 'n', 'Ñ': 'N'\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def mark_negation_context(words, window=3):\n",
    "    \"\"\"Marca palabras después de negaciones: no bueno -> no NEG_bueno\"\"\"\n",
    "    result = []\n",
    "    negation_active = False\n",
    "    steps_since_negation = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() in NEGATIONS:\n",
    "            result.append(word)\n",
    "            negation_active = True\n",
    "            steps_since_negation = 0\n",
    "        elif negation_active:\n",
    "            if steps_since_negation < window and word not in {'.', ',', ';', '!', '?'}:\n",
    "                result.append(f\"NEG_{word}\")\n",
    "                steps_since_negation += 1\n",
    "            else:\n",
    "                result.append(word)\n",
    "                negation_active = False\n",
    "        else:\n",
    "            result.append(word)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43161156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Así como va la cosa, por la hipersensibilidad de esta generación de cristal buena para nada y que todo lo consideran microagresion producto de la supremacía del hombre blanco, no queda mucho para que el contenido que se encuentre luego en la Deep Web, Sean cosas tan \"ofensivas\" .\n",
      "Procesado: asi va cosa hipersensibilidad generacion cristal buena nada NEG_y NEG_que NEG_todo consideran microagresion producto supremacia hombre blanco no NEG_queda NEG_mucho NEG_para contenido encuentre luego deep web cosas tan ofensivas\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_advanced(text, remove_stopwords=True, mark_negations=True):\n",
    "    \"\"\"Preprocesamiento avanzado con todas las mejoras\"\"\"\n",
    "    # Normalizar elongaciones\n",
    "    text = normalize_elongations(text)\n",
    "    \n",
    "    # Reemplazar patrones con tokens especiales\n",
    "    text = URL_PATTERN.sub(' URL ', text)\n",
    "    text = MENTION_PATTERN.sub(' MENTION ', text)\n",
    "    text = HASHTAG_PATTERN.sub(r' HASHTAG_\\1 ', text)\n",
    "    \n",
    "    # Reemplazar emojis usando librería emoji (más completo que regex)\n",
    "    text = emoji.replace_emoji(text, replace=' EMOJI ')\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover acentos\n",
    "    text = remove_accents(text)\n",
    "    \n",
    "    # Remover puntuación excepto ! y ?\n",
    "    text = re.sub(r'[^a-z0-9\\s!?_]', ' ', text)\n",
    "    \n",
    "    # Tokenizar\n",
    "    words = text.split()\n",
    "    \n",
    "    # Marcar negaciones\n",
    "    if mark_negations:\n",
    "        words = mark_negation_context(words)\n",
    "    \n",
    "    # Remover stopwords (excepto negaciones y palabras marcadas como NEG_)\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in SPANISH_STOPWORDS or w in NEGATIONS or w.startswith('NEG_')]\n",
    "    \n",
    "    # Normalizar espacios\n",
    "    text = ' '.join(words)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test\n",
    "sample = train_df['tweet'].iloc[100]\n",
    "print(\"Original:\", sample)\n",
    "print(\"Procesado:\", preprocess_text_advanced(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033de47b",
   "metadata": {},
   "source": [
    "## Extracción de Features Mejorada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45ffb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_advanced(text):\n",
    "    \"\"\"Extrae features avanzadas\"\"\"\n",
    "    # Contar patrones\n",
    "    n_urls = len(URL_PATTERN.findall(text))\n",
    "    n_mentions = len(MENTION_PATTERN.findall(text))\n",
    "    n_hashtags = len(HASHTAG_PATTERN.findall(text))\n",
    "    # Usar librería emoji para contar emojis (más preciso que regex)\n",
    "    n_emojis = emoji.emoji_count(text)\n",
    "    \n",
    "    # Contadores\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    char_count = len(text)\n",
    "    avg_word_length = char_count / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Mayúsculas\n",
    "    caps_ratio = sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0\n",
    "    n_caps_words = sum(1 for w in words if w.isupper() and len(w) > 1)\n",
    "    \n",
    "    # Puntuación\n",
    "    n_exclamations = text.count('!')\n",
    "    n_questions = text.count('?')\n",
    "    \n",
    "    # Elongaciones (detectar antes de normalizar)\n",
    "    n_elongations = len(ELONGATION_PATTERN.findall(text))\n",
    "    \n",
    "    # Negaciones\n",
    "    n_negations = sum(1 for w in words if w.lower() in NEGATIONS)\n",
    "    \n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'n_caps_words': n_caps_words,\n",
    "        'n_urls': n_urls,\n",
    "        'n_mentions': n_mentions,\n",
    "        'n_hashtags': n_hashtags,\n",
    "        'n_emojis': n_emojis,\n",
    "        'n_exclamations': n_exclamations,\n",
    "        'n_questions': n_questions,\n",
    "        'n_elongations': n_elongations,\n",
    "        'n_negations': n_negations,\n",
    "        'has_url': int(n_urls > 0),\n",
    "        'has_hashtag': int(n_hashtags > 0),\n",
    "        'has_mention': int(n_mentions > 0),\n",
    "        'has_emoji': int(n_emojis > 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68d451",
   "metadata": {},
   "source": [
    "## Aplicar Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de5ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesando datos de entrenamiento...\n",
      "Features extraídas: 16\n",
      "Texto preprocesado: text_light, text_clean\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocesando datos de entrenamiento...\")\n",
    "\n",
    "# Extraer features del texto original\n",
    "train_features = pd.DataFrame([extract_features_advanced(t) for t in train_df['tweet']])\n",
    "test_features = pd.DataFrame([extract_features_advanced(t) for t in test_df['tweet']])\n",
    "\n",
    "# Crear versiones procesadas del texto\n",
    "train_df['text_light'] = train_df['tweet'].apply(lambda x: preprocess_text_advanced(x, remove_stopwords=False, mark_negations=False))\n",
    "train_df['text_clean'] = train_df['tweet'].apply(lambda x: preprocess_text_advanced(x, remove_stopwords=True, mark_negations=True))\n",
    "\n",
    "test_df['text_light'] = test_df['tweet'].apply(lambda x: preprocess_text_advanced(x, remove_stopwords=False, mark_negations=False))\n",
    "test_df['text_clean'] = test_df['tweet'].apply(lambda x: preprocess_text_advanced(x, remove_stopwords=True, mark_negations=True))\n",
    "\n",
    "# Agregar features\n",
    "train_df = pd.concat([train_df, train_features], axis=1)\n",
    "test_df = pd.concat([test_df, test_features], axis=1)\n",
    "\n",
    "# Agreement score\n",
    "train_df['task1_agreement'] = train_df['labels_task1'].apply(\n",
    "    lambda x: Counter(x).most_common(1)[0][1] / len(x) if len(x) > 0 else 0.5\n",
    ")\n",
    "\n",
    "print(f\"Features extraídas: {len(train_features.columns)}\")\n",
    "print(f\"Texto preprocesado: text_light, text_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df976e",
   "metadata": {},
   "source": [
    "## Train/Val Split Estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30d31d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5154 (85.0%)\n",
      "Val: 910 (15.0%)\n",
      "Test: 934\n",
      "\n",
      "Distribución de clases:\n",
      "Train: task1\n",
      "NO     0.555297\n",
      "YES    0.444703\n",
      "Name: proportion, dtype: float64\n",
      "Val: task1\n",
      "NO     0.554945\n",
      "YES    0.445055\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split estratificado\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.15,\n",
    "    stratify=train_df['task1'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_data)} ({len(train_data)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Val: {len(val_data)} ({len(val_data)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_df)}\")\n",
    "\n",
    "print(\"\\nDistribución de clases:\")\n",
    "print(f\"Train: {train_data['task1'].value_counts(normalize=True)}\")\n",
    "print(f\"Val: {val_data['task1'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e3d1b",
   "metadata": {},
   "source": [
    "## Exportar Datos Preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1781a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('..') / 'preprocessed_data'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Columnas a guardar\n",
    "feature_cols = list(train_features.columns)\n",
    "columns_to_keep = [\n",
    "    'id_EXIST', 'tweet', 'text_light', 'text_clean',\n",
    "    'task1', 'task2', 'task3', 'task1_agreement'\n",
    "] + feature_cols\n",
    "\n",
    "test_columns = ['id_EXIST', 'tweet', 'text_light', 'text_clean'] + feature_cols\n",
    "\n",
    "# Exportar\n",
    "train_data[columns_to_keep].to_json(output_dir / 'train_preprocessed_v2.json', orient='records', indent=2, force_ascii=False)\n",
    "val_data[columns_to_keep].to_json(output_dir / 'val_preprocessed_v2.json', orient='records', indent=2, force_ascii=False)\n",
    "test_df[test_columns].to_json(output_dir / 'test_preprocessed_v2.json', orient='records', indent=2, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RFA2526pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
