{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f235c94",
   "metadata": {},
   "source": [
    "# Ensemble de Modelos LLM (Validación y Test)\n",
    "Combina las predicciones de los mejores modelos mediante votación mayoritaria.\n",
    "Procesa tanto el conjunto de validación como el de test por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbf9c0",
   "metadata": {},
   "source": [
    "## 1. Configuración y Carga de Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar gold standard de validación\n",
    "with open('../preprocessed_data/val_preprocessed_v2.json', 'r', encoding='utf-8') as f:\n",
    "    val_gold_data = json.load(f)\n",
    "val_gold_dict = {str(item['id_EXIST']): item['task1'] for item in val_gold_data}\n",
    "\n",
    "# Cargar datos de test (sin labels)\n",
    "with open('../preprocessed_data/test_preprocessed_v2.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "test_ids = {str(item['id_EXIST']) for item in test_data}\n",
    "\n",
    "print(f\"Validation samples: {len(val_gold_dict)}\")\n",
    "print(f\"Test samples: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de modelos - buscar en results_v2\n",
    "# Estructura: results_v2/{MODEL_NAME}/predictions/{dev_predictions_temp.json, BeingChillingWeWillWin_*.json}\n",
    "model_configs = [\n",
    "    {'name': 'Ministral8B_ft', 'path': '../results_v2/Ministral8B_ft/predictions'},\n",
    "    {'name': 'Ministral8B', 'path': '../results_v2/Ministral8B/predictions'},\n",
    "    {'name': 'f2llm4B', 'path': '../results_v2/F2LLM-4B/predictions'},\n",
    "    {'name': 'f2llm4B_clean', 'path': '../results_v2/F2LLM-4B_clean/predictions'},\n",
    "    {'name': 'KaLM', 'path': '../results_v2/KaLM/predictions'},\n",
    "    {'name': 'KaLM_clean', 'path': '../results_v2/KaLM_clean/predictions'}\n",
    "]\n",
    "\n",
    "def load_predictions(model_configs, split='val'):\n",
    "    \"\"\"Load predictions for validation or test split\"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    for model_cfg in model_configs:\n",
    "        model_name = model_cfg['name']\n",
    "        pred_dir = model_cfg['path']\n",
    "        \n",
    "        # Intentar cargar predicciones\n",
    "        if split == 'val':\n",
    "            # Buscar dev_predictions_temp.json o val_predictions*.json\n",
    "            candidates = [\n",
    "                os.path.join(pred_dir, 'dev_predictions_temp.json'),\n",
    "                os.path.join(pred_dir, 'val_predictions_temp.json'),\n",
    "                os.path.join(pred_dir, f'BeingChillingWeWillWin_{model_name}_val.json')\n",
    "            ]\n",
    "        else:  # test\n",
    "            # Buscar archivos de test\n",
    "            candidates = [\n",
    "                os.path.join(pred_dir, f'BeingChillingWeWillWin_{model_name}.json'),\n",
    "                os.path.join(pred_dir, f'{model_name}_test.json')\n",
    "            ]\n",
    "        \n",
    "        loaded = False\n",
    "        for path in candidates:\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    predictions[model_name] = {str(item['id']): item['value'] for item in data}\n",
    "                    print(f\"✓ Loaded {model_name} {split}: {len(predictions[model_name])} samples from {os.path.basename(path)}\")\n",
    "                    loaded = True\n",
    "                    break\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            except (KeyError, json.JSONDecodeError) as e:\n",
    "                print(f\"✗ Error loading {path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not loaded:\n",
    "            print(f\"✗ Could not find predictions for {model_name} ({split})\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Cargar predicciones de validación y test\n",
    "val_predictions = load_predictions(model_configs, split='val')\n",
    "test_predictions = load_predictions(model_configs, split='test')\n",
    "\n",
    "print(f\"\\n{len(val_predictions)} models loaded for validation\")\n",
    "print(f\"{len(test_predictions)} models loaded for test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bec09",
   "metadata": {},
   "source": [
    "## 2. Evaluar Modelos Individuales en Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c854a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No matching IDs between predictions and gold standard (test vs validation set)\n"
     ]
    }
   ],
   "source": [
    "val_results = []\n",
    "\n",
    "for name, preds in val_predictions.items():\n",
    "    common_ids = set(preds.keys()) & set(val_gold_dict.keys())\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(f\"Warning: No matching IDs for {name} in validation set\")\n",
    "        continue\n",
    "    \n",
    "    y_true = [val_gold_dict[id_] for id_ in common_ids]\n",
    "    y_pred = [preds[id_] for id_ in common_ids]\n",
    "    \n",
    "    val_results.append({\n",
    "        'Model': name,\n",
    "        'Samples': len(common_ids),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, pos_label='YES', zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, pos_label='YES', zero_division=0),\n",
    "        'F1': f1_score(y_true, y_pred, pos_label='YES', zero_division=0)\n",
    "    })\n",
    "\n",
    "if len(val_results) > 0:\n",
    "    df_val_results = pd.DataFrame(val_results).sort_values('F1', ascending=False)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION - Individual Model Results:\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_val_results.to_string(index=False))\n",
    "else:\n",
    "    df_val_results = pd.DataFrame()\n",
    "    print(\"Warning: No validation results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88972ca5",
   "metadata": {},
   "source": [
    "## 3. Ensemble por Votación Mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6115309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_ensemble(predictions_dict, ids):\n",
    "    \"\"\"Apply majority voting ensemble\"\"\"\n",
    "    ensemble_preds = {}\n",
    "    for id_ in ids:\n",
    "        votes = [preds[id_] for preds in predictions_dict.values() if id_ in preds]\n",
    "        if votes:\n",
    "            ensemble_preds[id_] = Counter(votes).most_common(1)[0][0]\n",
    "    return ensemble_preds\n",
    "\n",
    "def evaluate_ensemble(predictions_dict, gold_dict, config_models):\n",
    "    \"\"\"Evaluate ensemble configuration\"\"\"\n",
    "    filtered_preds = {k: v for k, v in predictions_dict.items() if k in config_models}\n",
    "    \n",
    "    if len(filtered_preds) == 0:\n",
    "        return None\n",
    "    \n",
    "    common_ids = set(filtered_preds[list(filtered_preds.keys())[0]].keys())\n",
    "    for preds in filtered_preds.values():\n",
    "        common_ids &= set(preds.keys())\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        return None\n",
    "    \n",
    "    ensemble_preds = voting_ensemble(filtered_preds, common_ids)\n",
    "    gold_ids = set(gold_dict.keys()) & common_ids\n",
    "    \n",
    "    if len(gold_ids) == 0:\n",
    "        return {\n",
    "            'ensemble_preds': ensemble_preds,\n",
    "            'metrics': None\n",
    "        }\n",
    "    \n",
    "    y_true = [gold_dict[id_] for id_ in gold_ids]\n",
    "    y_pred = [ensemble_preds[id_] for id_ in gold_ids]\n",
    "    \n",
    "    return {\n",
    "        'ensemble_preds': ensemble_preds,\n",
    "        'metrics': {\n",
    "            'Samples': len(gold_ids),\n",
    "            'Accuracy': accuracy_score(y_true, y_pred),\n",
    "            'Precision': precision_score(y_true, y_pred, pos_label='YES', zero_division=0),\n",
    "            'Recall': recall_score(y_true, y_pred, pos_label='YES', zero_division=0),\n",
    "            'F1': f1_score(y_true, y_pred, pos_label='YES', zero_division=0)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones de ensemble\n",
    "ensemble_configs = [\n",
    "    {'name': 'All_Models', 'models': list(val_predictions.keys())},\n",
    "    {'name': 'Top_3_Best_F1', 'models': df_val_results.head(3)['Model'].tolist() if len(df_val_results) >= 3 else list(val_predictions.keys())[:3]},\n",
    "    {'name': 'Ministral_Only', 'models': [k for k in val_predictions.keys() if 'Ministral' in k or 'ministral' in k]},\n",
    "    {'name': 'Fine_tuned_Models', 'models': [k for k in val_predictions.keys() if 'ft' in k or 'clean' in k]}\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ensemble Configurations:\")\n",
    "print(\"=\"*70)\n",
    "for cfg in ensemble_configs:\n",
    "    print(f\"{cfg['name']}: {cfg['models']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504751ed",
   "metadata": {},
   "source": [
    "## 4. Evaluar Ensembles en Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53550c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid results with F1 scores\n"
     ]
    }
   ],
   "source": [
    "val_ensemble_results = []\n",
    "val_ensemble_preds_dict = {}\n",
    "\n",
    "for config in ensemble_configs:\n",
    "    if not config['models']:\n",
    "        continue\n",
    "    \n",
    "    result = evaluate_ensemble(val_predictions, val_gold_dict, config['models'])\n",
    "    \n",
    "    if result is None:\n",
    "        continue\n",
    "    \n",
    "    val_ensemble_preds_dict[config['name']] = result['ensemble_preds']\n",
    "    \n",
    "    if result['metrics'] is not None:\n",
    "        val_ensemble_results.append({\n",
    "            'Ensemble': config['name'],\n",
    "            'N_Models': len(config['models']),\n",
    "            **result['metrics']\n",
    "        })\n",
    "\n",
    "if len(val_ensemble_results) > 0:\n",
    "    df_val_ensemble = pd.DataFrame(val_ensemble_results).sort_values('F1', ascending=False)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATION - Ensemble Results (Majority Voting):\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_val_ensemble.to_string(index=False))\n",
    "else:\n",
    "    df_val_ensemble = pd.DataFrame()\n",
    "    print(\"No validation ensemble results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c094f15",
   "metadata": {},
   "source": [
    "## 5. Generar Ensembles para Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1903b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common IDs found for confusion matrix\n"
     ]
    }
   ],
   "source": [
    "test_ensemble_preds_dict = {}\n",
    "\n",
    "for config in ensemble_configs:\n",
    "    if not config['models']:\n",
    "        continue\n",
    "    \n",
    "    # Filtrar solo los modelos que tienen predicciones de test\n",
    "    available_models = [m for m in config['models'] if m in test_predictions]\n",
    "    if len(available_models) == 0:\n",
    "        print(f\"No test predictions available for {config['name']}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_preds = {k: v for k, v in test_predictions.items() if k in available_models}\n",
    "    \n",
    "    # Encontrar IDs comunes\n",
    "    common_ids = set(filtered_preds[list(filtered_preds.keys())[0]].keys())\n",
    "    for preds in filtered_preds.values():\n",
    "        common_ids &= set(preds.keys())\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(f\"No common test IDs for {config['name']}\")\n",
    "        continue\n",
    "    \n",
    "    # Generar ensemble\n",
    "    ensemble_preds = voting_ensemble(filtered_preds, common_ids)\n",
    "    test_ensemble_preds_dict[config['name']] = ensemble_preds\n",
    "    \n",
    "    print(f\"✓ Generated test ensemble '{config['name']}': {len(ensemble_preds)} predictions\")\n",
    "\n",
    "print(f\"\\nTotal test ensembles created: {len(test_ensemble_preds_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558894e5",
   "metadata": {},
   "source": [
    "## 6. Comparación Validación: Individual vs Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for agreement analysis\n"
     ]
    }
   ],
   "source": [
    "if len(df_val_results) > 0 or len(df_val_ensemble) > 0:\n",
    "    dfs_to_concat = []\n",
    "    if len(df_val_results) > 0:\n",
    "        dfs_to_concat.append(df_val_results.assign(Type='Individual'))\n",
    "    if len(df_val_ensemble) > 0:\n",
    "        dfs_to_concat.append(df_val_ensemble.rename(columns={'Ensemble': 'Model', 'N_Models': 'Note'}).assign(Type='Ensemble'))\n",
    "    \n",
    "    all_val_results = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    for col in numeric_cols:\n",
    "        if col in all_val_results.columns:\n",
    "            all_val_results[col] = pd.to_numeric(all_val_results[col], errors='coerce')\n",
    "    \n",
    "    all_val_results = all_val_results.dropna(subset=['F1'])\n",
    "    \n",
    "    if len(all_val_results) > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"VALIDATION - Top 10 Models (Individual + Ensemble):\")\n",
    "        print(\"=\"*70)\n",
    "        print(all_val_results.nlargest(10, 'F1')[['Type', 'Model', 'F1', 'Accuracy', 'Precision', 'Recall']].to_string(index=False))\n",
    "        \n",
    "        # Visualización\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        data_to_plot = all_val_results.nlargest(10, 'F1')\n",
    "        x = range(len(data_to_plot))\n",
    "        plt.bar(x, data_to_plot['F1'], alpha=0.7, color=['green' if t == 'Ensemble' else 'blue' for t in data_to_plot['Type']])\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('Validation F1 Score: Individual vs Ensemble')\n",
    "        plt.xticks(x, data_to_plot['Model'], rotation=45, ha='right')\n",
    "        plt.legend(['Ensemble', 'Individual'])\n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid validation results with F1 scores\")\n",
    "else:\n",
    "    print(\"No validation results available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a18293",
   "metadata": {},
   "source": [
    "## 7. Matriz de Confusión del Mejor Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7e1c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../results/BeingChillingWeWillWin_ensemble_All_Models.json\n",
      "Models: Ministral8B_ft, Ministral8B, f2llm4B, f2llm4B_clean, KaLM, KaLM_clean\n",
      "Predictions: 934\n"
     ]
    }
   ],
   "source": [
    "if len(df_val_ensemble) > 0 and df_val_ensemble['F1'].notna().any():\n",
    "    best_idx = df_val_ensemble['F1'].idxmax()\n",
    "    best_config_name = df_val_ensemble.iloc[best_idx]['Ensemble']\n",
    "    best_config = next((cfg for cfg in ensemble_configs if cfg['name'] == best_config_name), None)\n",
    "else:\n",
    "    best_config = ensemble_configs[0] if len(ensemble_configs) > 0 else None\n",
    "\n",
    "if best_config is None or best_config['name'] not in val_ensemble_preds_dict:\n",
    "    print(\"No ensemble predictions available for confusion matrix\")\n",
    "else:\n",
    "    ensemble_preds = val_ensemble_preds_dict[best_config['name']]\n",
    "    common_ids = set(ensemble_preds.keys()) & set(val_gold_dict.keys())\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(\"No common IDs found for confusion matrix\")\n",
    "    else:\n",
    "        y_true = [val_gold_dict[id_] for id_ in common_ids]\n",
    "        y_pred = [ensemble_preds[id_] for id_ in common_ids]\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred, labels=['NO', 'YES'])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NO', 'YES'], yticklabels=['NO', 'YES'])\n",
    "        plt.title(f'Confusion Matrix: {best_config[\"name\"]} (Validation)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nClassification Report - {best_config['name']}:\")\n",
    "        print(classification_report(y_true, y_pred, labels=['NO', 'YES']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a669c",
   "metadata": {},
   "source": [
    "## 8. Guardar Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69f90150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot generate summary: missing individual or ensemble results\n"
     ]
    }
   ],
   "source": [
    "# Crear directorios de salida\n",
    "os.makedirs('../results_v2/ensemble/val', exist_ok=True)\n",
    "os.makedirs('../results_v2/ensemble/test', exist_ok=True)\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "# Guardar ensembles de validación\n",
    "for config_name, preds in val_ensemble_preds_dict.items():\n",
    "    if len(preds) > 0:\n",
    "        output_data = [{'test_case': 'EXIST2025', 'id': id_, 'value': pred} for id_, pred in preds.items()]\n",
    "        output_path = f'../results_v2/ensemble/val/BeingChillingWeWillWin_ensemble_{config_name}_val.json'\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        saved_files.append(output_path)\n",
    "        print(f\"✓ Saved validation: {output_path} ({len(preds)} predictions)\")\n",
    "\n",
    "# Guardar ensembles de test\n",
    "for config_name, preds in test_ensemble_preds_dict.items():\n",
    "    if len(preds) > 0:\n",
    "        output_data = [{'test_case': 'EXIST2025', 'id': id_, 'value': pred} for id_, pred in preds.items()]\n",
    "        output_path = f'../results_v2/ensemble/test/BeingChillingWeWillWin_ensemble_{config_name}.json'\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        saved_files.append(output_path)\n",
    "        print(f\"✓ Saved test: {output_path} ({len(preds)} predictions)\")\n",
    "\n",
    "print(f\"\\nTotal files saved: {len(saved_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa707480",
   "metadata": {},
   "source": [
    "## 9. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individual = df_val_results.iloc[0] if len(df_val_results) > 0 else None\n",
    "best_ensemble = df_val_ensemble.iloc[0] if len(df_val_ensemble) > 0 else None\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY - VALIDATION SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if best_individual is not None:\n",
    "    print(f\"\\nBest Individual Model: {best_individual['Model']}\")\n",
    "    print(f\"  F1: {best_individual['F1']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_individual['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_individual['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_individual['Recall']:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo individual model results available\")\n",
    "\n",
    "if best_ensemble is not None:\n",
    "    print(f\"\\nBest Ensemble: {best_ensemble['Ensemble']}\")\n",
    "    print(f\"  Models: {best_ensemble['N_Models']}\")\n",
    "    print(f\"  F1: {best_ensemble['F1']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_ensemble['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_ensemble['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_ensemble['Recall']:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo ensemble results available\")\n",
    "\n",
    "if best_individual is not None and best_ensemble is not None:\n",
    "    improvement = best_ensemble['F1'] - best_individual['F1']\n",
    "    improvement_pct = (improvement / best_individual['F1'] * 100) if best_individual['F1'] > 0 else 0\n",
    "    print(f\"\\nEnsemble Improvement over Best Individual:\")\n",
    "    print(f\"  F1 gain: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "    print(f\"  Status: {'✓ MEJORA' if improvement > 0 else '✗ NO mejora'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Validation ensembles saved: {len(val_ensemble_preds_dict)}\")\n",
    "print(f\"Test ensembles saved: {len(test_ensemble_preds_dict)}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
