==========================================
Starting execution of: 03_f2llm_4B_ft_tweet.ipynb
==========================================
Error occurred. Notebook saved to: entregables/03_f2llm_4B_ft_tweet_error.ipynb
Error: An error occurred while executing the following cell:
------------------
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=5,
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    logging_dir=LOGGING_DIR,
    logging_steps=50,
    eval_strategy='epoch',
    save_strategy="epoch",  # Guardar por Ã©poca
    save_total_limit=1,  # Solo mantener el mejor checkpoint
    metric_for_best_model="f1",
    load_best_model_at_end=True,
    bf16=True,
    lr_scheduler_type="cosine"
)

trainer = BinaryTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

model.save_pretrained(SAVE_PATH)
tokenizer.save_pretrained(SAVE_PATH)
------------------


[31m---------------------------------------------------------------------------[39m
[31mOutOfMemoryError[39m                          Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[6][39m[32m, line 27[39m
[32m      1[39m training_args = TrainingArguments(
[32m      2[39m     output_dir=OUTPUT_DIR,
[32m      3[39m     num_train_epochs=[32m5[39m,
[32m   (...)[39m[32m     16[39m     lr_scheduler_type=[33m"[39m[33mcosine[39m[33m"[39m
[32m     17[39m )
[32m     19[39m trainer = BinaryTrainer(
[32m     20[39m     model=model,
[32m     21[39m     args=training_args,
[32m   (...)[39m[32m     24[39m     compute_metrics=compute_metrics,
[32m     25[39m )
[32m---> [39m[32m27[39m [43mtrainer[49m[43m.[49m[43mtrain[49m[43m([49m[43m)[49m
[32m     29[39m model.save_pretrained(SAVE_PATH)
[32m     30[39m tokenizer.save_pretrained(SAVE_PATH)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/trainer.py:2325[39m, in [36mTrainer.train[39m[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[39m
[32m   2323[39m         hf_hub_utils.enable_progress_bars()
[32m   2324[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m2325[39m     [38;5;28;01mreturn[39;00m [43minner_training_loop[49m[43m([49m
[32m   2326[39m [43m        [49m[43margs[49m[43m=[49m[43margs[49m[43m,[49m
[32m   2327[39m [43m        [49m[43mresume_from_checkpoint[49m[43m=[49m[43mresume_from_checkpoint[49m[43m,[49m
[32m   2328[39m [43m        [49m[43mtrial[49m[43m=[49m[43mtrial[49m[43m,[49m
[32m   2329[39m [43m        [49m[43mignore_keys_for_eval[49m[43m=[49m[43mignore_keys_for_eval[49m[43m,[49m
[32m   2330[39m [43m    [49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/trainer.py:2674[39m, in [36mTrainer._inner_training_loop[39m[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[39m
[32m   2667[39m context = (
[32m   2668[39m     functools.partial([38;5;28mself[39m.accelerator.no_sync, model=model)
[32m   2669[39m     [38;5;28;01mif[39;00m i != [38;5;28mlen[39m(batch_samples) - [32m1[39m
[32m   2670[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m.accelerator.distributed_type != DistributedType.DEEPSPEED
[32m   2671[39m     [38;5;28;01melse[39;00m contextlib.nullcontext
[32m   2672[39m )
[32m   2673[39m [38;5;28;01mwith[39;00m context():
[32m-> [39m[32m2674[39m     tr_loss_step = [38;5;28;43mself[39;49m[43m.[49m[43mtraining_step[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[43mnum_items_in_batch[49m[43m)[49m
[32m   2676[39m [38;5;28;01mif[39;00m (
[32m   2677[39m     args.logging_nan_inf_filter
[32m   2678[39m     [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torch_xla_available()
[32m   2679[39m     [38;5;129;01mand[39;00m (torch.isnan(tr_loss_step) [38;5;129;01mor[39;00m torch.isinf(tr_loss_step))
[32m   2680[39m ):
[32m   2681[39m     [38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses[39;00m
[32m   2682[39m     tr_loss = tr_loss + tr_loss / ([32m1[39m + [38;5;28mself[39m.state.global_step - [38;5;28mself[39m._globalstep_last_logged)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/trainer.py:4020[39m, in [36mTrainer.training_step[39m[34m(self, model, inputs, num_items_in_batch)[39m
[32m   4017[39m     [38;5;28;01mreturn[39;00m loss_mb.reduce_mean().detach().to([38;5;28mself[39m.args.device)
[32m   4019[39m [38;5;28;01mwith[39;00m [38;5;28mself[39m.compute_loss_context_manager():
[32m-> [39m[32m4020[39m     loss = [38;5;28;43mself[39;49m[43m.[49m[43mcompute_loss[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[43mnum_items_in_batch[49m[43m=[49m[43mnum_items_in_batch[49m[43m)[49m
[32m   4022[39m [38;5;28;01mdel[39;00m inputs
[32m   4023[39m [38;5;28;01mif[39;00m (
[32m   4024[39m     [38;5;28mself[39m.args.torch_empty_cache_steps [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
[32m   4025[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m.state.global_step % [38;5;28mself[39m.args.torch_empty_cache_steps == [32m0[39m
[32m   4026[39m ):

[36mCell[39m[36m [39m[32mIn[5][39m[32m, line 49[39m, in [36mBinaryTrainer.compute_loss[39m[34m(self, model, inputs, return_outputs, **kwargs)[39m
[32m     47[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mcompute_loss[39m([38;5;28mself[39m, model, inputs, return_outputs=[38;5;28;01mFalse[39;00m, **kwargs):
[32m     48[39m     labels = inputs.pop([33m"[39m[33mlabels[39m[33m"[39m).float()
[32m---> [39m[32m49[39m     outputs = [43mmodel[49m[43m([49m[43m*[49m[43m*[49m[43minputs[49m[43m)[49m
[32m     50[39m     logits = outputs.logits.squeeze(-[32m1[39m)
[32m     51[39m     loss_fn = nn.BCEWithLogitsLoss()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/accelerate/utils/operations.py:819[39m, in [36mconvert_outputs_to_fp32.<locals>.forward[39m[34m(*args, **kwargs)[39m
[32m    818[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mforward[39m(*args, **kwargs):
[32m--> [39m[32m819[39m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/accelerate/utils/operations.py:807[39m, in [36mConvertOutputsToFp32.__call__[39m[34m(self, *args, **kwargs)[39m
[32m    806[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__call__[39m([38;5;28mself[39m, *args, **kwargs):
[32m--> [39m[32m807[39m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[43m.[49m[43mmodel_forward[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44[39m, in [36mautocast_decorator.<locals>.decorate_autocast[39m[34m(*args, **kwargs)[39m
[32m     41[39m [38;5;129m@functools[39m.wraps(func)
[32m     42[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mdecorate_autocast[39m(*args, **kwargs):
[32m     43[39m     [38;5;28;01mwith[39;00m autocast_instance:
[32m---> [39m[32m44[39m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/peft/peft_model.py:1722[39m, in [36mPeftModelForSequenceClassification.forward[39m[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)[39m
[32m   1720[39m         [38;5;28;01mif[39;00m peft_config.peft_type == PeftType.POLY:
[32m   1721[39m             kwargs[[33m"[39m[33mtask_ids[39m[33m"[39m] = task_ids
[32m-> [39m[32m1722[39m         [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43mbase_model[49m[43m([49m
[32m   1723[39m [43m            [49m[43minput_ids[49m[43m=[49m[43minput_ids[49m[43m,[49m
[32m   1724[39m [43m            [49m[43mattention_mask[49m[43m=[49m[43mattention_mask[49m[43m,[49m
[32m   1725[39m [43m            [49m[43minputs_embeds[49m[43m=[49m[43minputs_embeds[49m[43m,[49m
[32m   1726[39m [43m            [49m[43mlabels[49m[43m=[49m[43mlabels[49m[43m,[49m
[32m   1727[39m [43m            [49m[43moutput_attentions[49m[43m=[49m[43moutput_attentions[49m[43m,[49m
[32m   1728[39m [43m            [49m[43moutput_hidden_states[49m[43m=[49m[43moutput_hidden_states[49m[43m,[49m
[32m   1729[39m [43m            [49m[43mreturn_dict[49m[43m=[49m[43mreturn_dict[49m[43m,[49m
[32m   1730[39m [43m            [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m   1731[39m [43m        [49m[43m)[49m
[32m   1733[39m batch_size = _get_batch_size(input_ids, inputs_embeds)
[32m   1734[39m [38;5;28;01mif[39;00m attention_mask [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
[32m   1735[39m     [38;5;66;03m# concat prompt attention mask[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:308[39m, in [36mBaseTuner.forward[39m[34m(self, *args, **kwargs)[39m
[32m    307[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mforward[39m([38;5;28mself[39m, *args: Any, **kwargs: Any):
[32m--> [39m[32m308[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43mmodel[49m[43m.[49m[43mforward[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/generic.py:918[39m, in [36mcan_return_tuple.<locals>.wrapper[39m[34m(self, *args, **kwargs)[39m
[32m    916[39m [38;5;28;01mif[39;00m return_dict_passed [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
[32m    917[39m     return_dict = return_dict_passed
[32m--> [39m[32m918[39m output = [43mfunc[49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m    919[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m return_dict [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28misinstance[39m(output, [38;5;28mtuple[39m):
[32m    920[39m     output = output.to_tuple()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/modeling_layers.py:124[39m, in [36mGenericForSequenceClassification.forward[39m[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, **kwargs)[39m
[32m    111[39m [38;5;129m@can_return_tuple[39m
[32m    112[39m [38;5;129m@auto_docstring[39m
[32m    113[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mforward[39m(
[32m   (...)[39m[32m    122[39m     **kwargs: Unpack[TransformersKwargs],
[32m    123[39m ) -> SequenceClassifierOutputWithPast:
[32m--> [39m[32m124[39m     transformer_outputs: BaseModelOutputWithPast = [38;5;28;43mgetattr[39;49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mbase_model_prefix[49m[43m)[49m[43m([49m
[32m    125[39m [43m        [49m[43minput_ids[49m[43m,[49m
[32m    126[39m [43m        [49m[43mattention_mask[49m[43m=[49m[43mattention_mask[49m[43m,[49m
[32m    127[39m [43m        [49m[43mposition_ids[49m[43m=[49m[43mposition_ids[49m[43m,[49m
[32m    128[39m [43m        [49m[43mpast_key_values[49m[43m=[49m[43mpast_key_values[49m[43m,[49m
[32m    129[39m [43m        [49m[43minputs_embeds[49m[43m=[49m[43minputs_embeds[49m[43m,[49m
[32m    130[39m [43m        [49m[43muse_cache[49m[43m=[49m[43muse_cache[49m[43m,[49m
[32m    131[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m    132[39m [43m    [49m[43m)[49m
[32m    133[39m     hidden_states = transformer_outputs.last_hidden_state
[32m    134[39m     logits = [38;5;28mself[39m.score(hidden_states)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/generic.py:1072[39m, in [36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper[39m[34m(self, *args, **kwargs)[39m
[32m   1069[39m                 monkey_patched_layers.append((module, original_forward))
[32m   1071[39m [38;5;28;01mtry[39;00m:
[32m-> [39m[32m1072[39m     outputs = [43mfunc[49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1073[39m [38;5;28;01mexcept[39;00m [38;5;167;01mTypeError[39;00m [38;5;28;01mas[39;00m original_exception:
[32m   1074[39m     [38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.[39;00m
[32m   1075[39m     [38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception[39;00m
[32m   1076[39m     [38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function[39;00m
[32m   1077[39m     kwargs_without_recordable = {k: v [38;5;28;01mfor[39;00m k, v [38;5;129;01min[39;00m kwargs.items() [38;5;28;01mif[39;00m k [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m recordable_keys}

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:410[39m, in [36mQwen3Model.forward[39m[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)[39m
[32m    407[39m position_embeddings = [38;5;28mself[39m.rotary_emb(hidden_states, position_ids)
[32m    409[39m [38;5;28;01mfor[39;00m decoder_layer [38;5;129;01min[39;00m [38;5;28mself[39m.layers[: [38;5;28mself[39m.config.num_hidden_layers]:
[32m--> [39m[32m410[39m     hidden_states = [43mdecoder_layer[49m[43m([49m
[32m    411[39m [43m        [49m[43mhidden_states[49m[43m,[49m
[32m    412[39m [43m        [49m[43mattention_mask[49m[43m=[49m[43mcausal_mask_mapping[49m[43m[[49m[43mdecoder_layer[49m[43m.[49m[43mattention_type[49m[43m][49m[43m,[49m
[32m    413[39m [43m        [49m[43mposition_ids[49m[43m=[49m[43mposition_ids[49m[43m,[49m
[32m    414[39m [43m        [49m[43mpast_key_values[49m[43m=[49m[43mpast_key_values[49m[43m,[49m
[32m    415[39m [43m        [49m[43muse_cache[49m[43m=[49m[43muse_cache[49m[43m,[49m
[32m    416[39m [43m        [49m[43mcache_position[49m[43m=[49m[43mcache_position[49m[43m,[49m
[32m    417[39m [43m        [49m[43mposition_embeddings[49m[43m=[49m[43mposition_embeddings[49m[43m,[49m
[32m    418[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m    419[39m [43m    [49m[43m)[49m
[32m    421[39m hidden_states = [38;5;28mself[39m.norm(hidden_states)
[32m    422[39m [38;5;28;01mreturn[39;00m BaseModelOutputWithPast(
[32m    423[39m     last_hidden_state=hidden_states,
[32m    424[39m     past_key_values=past_key_values [38;5;28;01mif[39;00m use_cache [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[32m    425[39m )

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/modeling_layers.py:94[39m, in [36mGradientCheckpointingLayer.__call__[39m[34m(self, *args, **kwargs)[39m
[32m     91[39m         logger.warning_once(message)
[32m     93[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._gradient_checkpointing_func(partial([38;5;28msuper[39m().[34m__call__[39m, **kwargs), *args)
[32m---> [39m[32m94[39m [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__call__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/deprecation.py:172[39m, in [36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func[39m[34m(*args, **kwargs)[39m
[32m    168[39m [38;5;28;01melif[39;00m minimum_action [38;5;129;01min[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torchdynamo_compiling():
[32m    169[39m     [38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead[39;00m
[32m    170[39m     warnings.warn(message, [38;5;167;01mFutureWarning[39;00m, stacklevel=[32m2[39m)
[32m--> [39m[32m172[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:260[39m, in [36mQwen3DecoderLayer.forward[39m[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)[39m
[32m    258[39m hidden_states = [38;5;28mself[39m.input_layernorm(hidden_states)
[32m    259[39m [38;5;66;03m# Self Attention[39;00m
[32m--> [39m[32m260[39m hidden_states, _ = [38;5;28;43mself[39;49m[43m.[49m[43mself_attn[49m[43m([49m
[32m    261[39m [43m    [49m[43mhidden_states[49m[43m=[49m[43mhidden_states[49m[43m,[49m
[32m    262[39m [43m    [49m[43mattention_mask[49m[43m=[49m[43mattention_mask[49m[43m,[49m
[32m    263[39m [43m    [49m[43mposition_ids[49m[43m=[49m[43mposition_ids[49m[43m,[49m
[32m    264[39m [43m    [49m[43mpast_key_values[49m[43m=[49m[43mpast_key_values[49m[43m,[49m
[32m    265[39m [43m    [49m[43muse_cache[49m[43m=[49m[43muse_cache[49m[43m,[49m
[32m    266[39m [43m    [49m[43mcache_position[49m[43m=[49m[43mcache_position[49m[43m,[49m
[32m    267[39m [43m    [49m[43mposition_embeddings[49m[43m=[49m[43mposition_embeddings[49m[43m,[49m
[32m    268[39m [43m    [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m    269[39m [43m[49m[43m)[49m
[32m    270[39m hidden_states = residual + hidden_states
[32m    272[39m [38;5;66;03m# Fully Connected[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/deprecation.py:172[39m, in [36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func[39m[34m(*args, **kwargs)[39m
[32m    168[39m [38;5;28;01melif[39;00m minimum_action [38;5;129;01min[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torchdynamo_compiling():
[32m    169[39m     [38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead[39;00m
[32m    170[39m     warnings.warn(message, [38;5;167;01mFutureWarning[39;00m, stacklevel=[32m2[39m)
[32m--> [39m[32m172[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:200[39m, in [36mQwen3Attention.forward[39m[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)[39m
[32m    197[39m input_shape = hidden_states.shape[:-[32m1[39m]
[32m    198[39m hidden_shape = (*input_shape, -[32m1[39m, [38;5;28mself[39m.head_dim)
[32m--> [39m[32m200[39m query_states = [38;5;28mself[39m.q_norm([38;5;28;43mself[39;49m[43m.[49m[43mq_proj[49m[43m([49m[43mhidden_states[49m[43m)[49m.view(hidden_shape)).transpose([32m1[39m, [32m2[39m)
[32m    201[39m key_states = [38;5;28mself[39m.k_norm([38;5;28mself[39m.k_proj(hidden_states).view(hidden_shape)).transpose([32m1[39m, [32m2[39m)
[32m    202[39m value_states = [38;5;28mself[39m.v_proj(hidden_states).view(hidden_shape).transpose([32m1[39m, [32m2[39m)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/peft/tuners/lora/layer.py:807[39m, in [36mLinear.forward[39m[34m(self, x, *args, **kwargs)[39m
[32m    805[39m x = [38;5;28mself[39m._cast_input_dtype(x, lora_A.weight.dtype)
[32m    806[39m [38;5;28;01mif[39;00m active_adapter [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mself[39m.lora_variant:  [38;5;66;03m# vanilla LoRA[39;00m
[32m--> [39m[32m807[39m     result = result + [43mlora_B[49m[43m([49m[43mlora_A[49m[43m([49m[43mdropout[49m[43m([49m[43mx[49m[43m)[49m[43m)[49m[43m)[49m[43m [49m[43m*[49m[43m [49m[43mscaling[49m
[32m    808[39m [38;5;28;01melse[39;00m:
[32m    809[39m     result = [38;5;28mself[39m.lora_variant[active_adapter].forward(
[32m    810[39m         [38;5;28mself[39m,
[32m    811[39m         active_adapter=active_adapter,
[32m   (...)[39m[32m    815[39m         **kwargs,
[32m    816[39m     )

[31mOutOfMemoryError[39m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 1.69 MiB is free. Process 3975170 has 8.02 GiB memory in use. Process 3983462 has 8.11 GiB memory in use. Including non-PyTorch memory, this process has 28.23 GiB memory in use. Of the allocated memory 27.47 GiB is allocated by PyTorch, and 271.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

WARNING: 03_f2llm_4B_ft_tweet.ipynb execution failed, continuing with next notebook...

==========================================
Starting execution of: 03_modelos_clasicos.ipynb
==========================================
Successfully saved to: entregables/03_modelos_clasicos_out.ipynb

==========================================
Starting execution of: 04_f2llm_4B_ft_text_clean.ipynb
==========================================
Error occurred. Notebook saved to: entregables/04_f2llm_4B_ft_text_clean_error.ipynb
Error: An error occurred while executing the following cell:
------------------
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=5,
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    logging_dir=LOGGING_DIR,
    logging_steps=50,
    eval_strategy='epoch',
    save_strategy="epoch",  # Guardar por Ã©poca
    save_total_limit=1,  # Solo mantener el mejor checkpoint
    metric_for_best_model="f1",
    load_best_model_at_end=True,
    bf16=True,
    lr_scheduler_type="cosine"
)

trainer = BinaryTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

model.save_pretrained(SAVE_PATH)
tokenizer.save_pretrained(SAVE_PATH)
------------------


[31m---------------------------------------------------------------------------[39m
[31mOutOfMemoryError[39m                          Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[6][39m[32m, line 27[39m
[32m      1[39m training_args = TrainingArguments(
[32m      2[39m     output_dir=OUTPUT_DIR,
[32m      3[39m     num_train_epochs=[32m5[39m,
[32m   (...)[39m[32m     16[39m     lr_scheduler_type=[33m"[39m[33mcosine[39m[33m"[39m
[32m     17[39m )
[32m     19[39m trainer = BinaryTrainer(
[32m     20[39m     model=model,
[32m     21[39m     args=training_args,
[32m   (...)[39m[32m     24[39m     compute_metrics=compute_metrics,
[32m     25[39m )
[32m---> [39m[32m27[39m [43mtrainer[49m[43m.[49m[43mtrain[49m[43m([49m[43m)[49m
[32m     29[39m model.save_pretrained(SAVE_PATH)
[32m     30[39m tokenizer.save_pretrained(SAVE_PATH)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/trainer.py:2325[39m, in [36mTrainer.train[39m[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[39m
[32m   2323[39m         hf_hub_utils.enable_progress_bars()
[32m   2324[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m2325[39m     [38;5;28;01mreturn[39;00m [43minner_training_loop[49m[43m([49m
[32m   2326[39m [43m        [49m[43margs[49m[43m=[49m[43margs[49m[43m,[49m
[32m   2327[39m [43m        [49m[43mresume_from_checkpoint[49m[43m=[49m[43mresume_from_checkpoint[49m[43m,[49m
[32m   2328[39m [43m        [49m[43mtrial[49m[43m=[49m[43mtrial[49m[43m,[49m
[32m   2329[39m [43m        [49m[43mignore_keys_for_eval[49m[43m=[49m[43mignore_keys_for_eval[49m[43m,[49m
[32m   2330[39m [43m    [49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/trainer.py:2674[39m, in [36mTrainer._inner_training_loop[39m[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[39m
[32m   2667[39m context = (
[32m   2668[39m     functools.partial([38;5;28mself[39m.accelerator.no_sync, model=model)
[32m   2669[39m     [38;5;28;01mif[39;00m i != [38;5;28mlen[39m(batch_samples) - [32m1[39m
[32m   2670[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m.accelerator.distributed_type != DistributedType.DEEPSPEED
[32m   2671[39m     [38;5;28;01melse[39;00m contextlib.nullcontext
[32m   2672[39m )
[32m   2673[39m [38;5;28;01mwith[39;00m context():
[32m-> [39m[32m2674[39m     tr_loss_step = [38;5;28;43mself[39;49m[43m.[49m[43mtraining_step[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[43mnum_items_in_batch[49m[43m)[49m
[32m   2676[39m [38;5;28;01mif[39;00m (
[32m   2677[39m     args.logging_nan_inf_filter
[32m   2678[39m     [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torch_xla_available()
[32m   2679[39m     [38;5;129;01mand[39;00m (torch.isnan(tr_loss_step) [38;5;129;01mor[39;00m torch.isinf(tr_loss_step))
[32m   2680[39m ):
[32m   2681[39m     [38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses[39;00m
[32m   2682[39m     tr_loss = tr_loss + tr_loss / ([32m1[39m + [38;5;28mself[39m.state.global_step - [38;5;28mself[39m._globalstep_last_logged)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/trainer.py:4020[39m, in [36mTrainer.training_step[39m[34m(self, model, inputs, num_items_in_batch)[39m
[32m   4017[39m     [38;5;28;01mreturn[39;00m loss_mb.reduce_mean().detach().to([38;5;28mself[39m.args.device)
[32m   4019[39m [38;5;28;01mwith[39;00m [38;5;28mself[39m.compute_loss_context_manager():
[32m-> [39m[32m4020[39m     loss = [38;5;28;43mself[39;49m[43m.[49m[43mcompute_loss[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[43mnum_items_in_batch[49m[43m=[49m[43mnum_items_in_batch[49m[43m)[49m
[32m   4022[39m [38;5;28;01mdel[39;00m inputs
[32m   4023[39m [38;5;28;01mif[39;00m (
[32m   4024[39m     [38;5;28mself[39m.args.torch_empty_cache_steps [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
[32m   4025[39m     [38;5;129;01mand[39;00m [38;5;28mself[39m.state.global_step % [38;5;28mself[39m.args.torch_empty_cache_steps == [32m0[39m
[32m   4026[39m ):

[36mCell[39m[36m [39m[32mIn[5][39m[32m, line 49[39m, in [36mBinaryTrainer.compute_loss[39m[34m(self, model, inputs, return_outputs, **kwargs)[39m
[32m     47[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mcompute_loss[39m([38;5;28mself[39m, model, inputs, return_outputs=[38;5;28;01mFalse[39;00m, **kwargs):
[32m     48[39m     labels = inputs.pop([33m"[39m[33mlabels[39m[33m"[39m).float()
[32m---> [39m[32m49[39m     outputs = [43mmodel[49m[43m([49m[43m*[49m[43m*[49m[43minputs[49m[43m)[49m
[32m     50[39m     logits = outputs.logits.squeeze(-[32m1[39m)
[32m     51[39m     loss_fn = nn.BCEWithLogitsLoss()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/accelerate/utils/operations.py:819[39m, in [36mconvert_outputs_to_fp32.<locals>.forward[39m[34m(*args, **kwargs)[39m
[32m    818[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mforward[39m(*args, **kwargs):
[32m--> [39m[32m819[39m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/accelerate/utils/operations.py:807[39m, in [36mConvertOutputsToFp32.__call__[39m[34m(self, *args, **kwargs)[39m
[32m    806[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__call__[39m([38;5;28mself[39m, *args, **kwargs):
[32m--> [39m[32m807[39m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[43m.[49m[43mmodel_forward[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44[39m, in [36mautocast_decorator.<locals>.decorate_autocast[39m[34m(*args, **kwargs)[39m
[32m     41[39m [38;5;129m@functools[39m.wraps(func)
[32m     42[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mdecorate_autocast[39m(*args, **kwargs):
[32m     43[39m     [38;5;28;01mwith[39;00m autocast_instance:
[32m---> [39m[32m44[39m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/peft/peft_model.py:1722[39m, in [36mPeftModelForSequenceClassification.forward[39m[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)[39m
[32m   1720[39m         [38;5;28;01mif[39;00m peft_config.peft_type == PeftType.POLY:
[32m   1721[39m             kwargs[[33m"[39m[33mtask_ids[39m[33m"[39m] = task_ids
[32m-> [39m[32m1722[39m         [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43mbase_model[49m[43m([49m
[32m   1723[39m [43m            [49m[43minput_ids[49m[43m=[49m[43minput_ids[49m[43m,[49m
[32m   1724[39m [43m            [49m[43mattention_mask[49m[43m=[49m[43mattention_mask[49m[43m,[49m
[32m   1725[39m [43m            [49m[43minputs_embeds[49m[43m=[49m[43minputs_embeds[49m[43m,[49m
[32m   1726[39m [43m            [49m[43mlabels[49m[43m=[49m[43mlabels[49m[43m,[49m
[32m   1727[39m [43m            [49m[43moutput_attentions[49m[43m=[49m[43moutput_attentions[49m[43m,[49m
[32m   1728[39m [43m            [49m[43moutput_hidden_states[49m[43m=[49m[43moutput_hidden_states[49m[43m,[49m
[32m   1729[39m [43m            [49m[43mreturn_dict[49m[43m=[49m[43mreturn_dict[49m[43m,[49m
[32m   1730[39m [43m            [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m   1731[39m [43m        [49m[43m)[49m
[32m   1733[39m batch_size = _get_batch_size(input_ids, inputs_embeds)
[32m   1734[39m [38;5;28;01mif[39;00m attention_mask [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
[32m   1735[39m     [38;5;66;03m# concat prompt attention mask[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:308[39m, in [36mBaseTuner.forward[39m[34m(self, *args, **kwargs)[39m
[32m    307[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mforward[39m([38;5;28mself[39m, *args: Any, **kwargs: Any):
[32m--> [39m[32m308[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43mmodel[49m[43m.[49m[43mforward[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/generic.py:918[39m, in [36mcan_return_tuple.<locals>.wrapper[39m[34m(self, *args, **kwargs)[39m
[32m    916[39m [38;5;28;01mif[39;00m return_dict_passed [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
[32m    917[39m     return_dict = return_dict_passed
[32m--> [39m[32m918[39m output = [43mfunc[49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m    919[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m return_dict [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28misinstance[39m(output, [38;5;28mtuple[39m):
[32m    920[39m     output = output.to_tuple()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/modeling_layers.py:124[39m, in [36mGenericForSequenceClassification.forward[39m[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, **kwargs)[39m
[32m    111[39m [38;5;129m@can_return_tuple[39m
[32m    112[39m [38;5;129m@auto_docstring[39m
[32m    113[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mforward[39m(
[32m   (...)[39m[32m    122[39m     **kwargs: Unpack[TransformersKwargs],
[32m    123[39m ) -> SequenceClassifierOutputWithPast:
[32m--> [39m[32m124[39m     transformer_outputs: BaseModelOutputWithPast = [38;5;28;43mgetattr[39;49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[43m.[49m[43mbase_model_prefix[49m[43m)[49m[43m([49m
[32m    125[39m [43m        [49m[43minput_ids[49m[43m,[49m
[32m    126[39m [43m        [49m[43mattention_mask[49m[43m=[49m[43mattention_mask[49m[43m,[49m
[32m    127[39m [43m        [49m[43mposition_ids[49m[43m=[49m[43mposition_ids[49m[43m,[49m
[32m    128[39m [43m        [49m[43mpast_key_values[49m[43m=[49m[43mpast_key_values[49m[43m,[49m
[32m    129[39m [43m        [49m[43minputs_embeds[49m[43m=[49m[43minputs_embeds[49m[43m,[49m
[32m    130[39m [43m        [49m[43muse_cache[49m[43m=[49m[43muse_cache[49m[43m,[49m
[32m    131[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m    132[39m [43m    [49m[43m)[49m
[32m    133[39m     hidden_states = transformer_outputs.last_hidden_state
[32m    134[39m     logits = [38;5;28mself[39m.score(hidden_states)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/generic.py:1072[39m, in [36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper[39m[34m(self, *args, **kwargs)[39m
[32m   1069[39m                 monkey_patched_layers.append((module, original_forward))
[32m   1071[39m [38;5;28;01mtry[39;00m:
[32m-> [39m[32m1072[39m     outputs = [43mfunc[49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1073[39m [38;5;28;01mexcept[39;00m [38;5;167;01mTypeError[39;00m [38;5;28;01mas[39;00m original_exception:
[32m   1074[39m     [38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.[39;00m
[32m   1075[39m     [38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception[39;00m
[32m   1076[39m     [38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function[39;00m
[32m   1077[39m     kwargs_without_recordable = {k: v [38;5;28;01mfor[39;00m k, v [38;5;129;01min[39;00m kwargs.items() [38;5;28;01mif[39;00m k [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m recordable_keys}

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:410[39m, in [36mQwen3Model.forward[39m[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)[39m
[32m    407[39m position_embeddings = [38;5;28mself[39m.rotary_emb(hidden_states, position_ids)
[32m    409[39m [38;5;28;01mfor[39;00m decoder_layer [38;5;129;01min[39;00m [38;5;28mself[39m.layers[: [38;5;28mself[39m.config.num_hidden_layers]:
[32m--> [39m[32m410[39m     hidden_states = [43mdecoder_layer[49m[43m([49m
[32m    411[39m [43m        [49m[43mhidden_states[49m[43m,[49m
[32m    412[39m [43m        [49m[43mattention_mask[49m[43m=[49m[43mcausal_mask_mapping[49m[43m[[49m[43mdecoder_layer[49m[43m.[49m[43mattention_type[49m[43m][49m[43m,[49m
[32m    413[39m [43m        [49m[43mposition_ids[49m[43m=[49m[43mposition_ids[49m[43m,[49m
[32m    414[39m [43m        [49m[43mpast_key_values[49m[43m=[49m[43mpast_key_values[49m[43m,[49m
[32m    415[39m [43m        [49m[43muse_cache[49m[43m=[49m[43muse_cache[49m[43m,[49m
[32m    416[39m [43m        [49m[43mcache_position[49m[43m=[49m[43mcache_position[49m[43m,[49m
[32m    417[39m [43m        [49m[43mposition_embeddings[49m[43m=[49m[43mposition_embeddings[49m[43m,[49m
[32m    418[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m    419[39m [43m    [49m[43m)[49m
[32m    421[39m hidden_states = [38;5;28mself[39m.norm(hidden_states)
[32m    422[39m [38;5;28;01mreturn[39;00m BaseModelOutputWithPast(
[32m    423[39m     last_hidden_state=hidden_states,
[32m    424[39m     past_key_values=past_key_values [38;5;28;01mif[39;00m use_cache [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[32m    425[39m )

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/modeling_layers.py:94[39m, in [36mGradientCheckpointingLayer.__call__[39m[34m(self, *args, **kwargs)[39m
[32m     91[39m         logger.warning_once(message)
[32m     93[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._gradient_checkpointing_func(partial([38;5;28msuper[39m().[34m__call__[39m, **kwargs), *args)
[32m---> [39m[32m94[39m [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__call__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/deprecation.py:172[39m, in [36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func[39m[34m(*args, **kwargs)[39m
[32m    168[39m [38;5;28;01melif[39;00m minimum_action [38;5;129;01min[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torchdynamo_compiling():
[32m    169[39m     [38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead[39;00m
[32m    170[39m     warnings.warn(message, [38;5;167;01mFutureWarning[39;00m, stacklevel=[32m2[39m)
[32m--> [39m[32m172[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:260[39m, in [36mQwen3DecoderLayer.forward[39m[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)[39m
[32m    258[39m hidden_states = [38;5;28mself[39m.input_layernorm(hidden_states)
[32m    259[39m [38;5;66;03m# Self Attention[39;00m
[32m--> [39m[32m260[39m hidden_states, _ = [38;5;28;43mself[39;49m[43m.[49m[43mself_attn[49m[43m([49m
[32m    261[39m [43m    [49m[43mhidden_states[49m[43m=[49m[43mhidden_states[49m[43m,[49m
[32m    262[39m [43m    [49m[43mattention_mask[49m[43m=[49m[43mattention_mask[49m[43m,[49m
[32m    263[39m [43m    [49m[43mposition_ids[49m[43m=[49m[43mposition_ids[49m[43m,[49m
[32m    264[39m [43m    [49m[43mpast_key_values[49m[43m=[49m[43mpast_key_values[49m[43m,[49m
[32m    265[39m [43m    [49m[43muse_cache[49m[43m=[49m[43muse_cache[49m[43m,[49m
[32m    266[39m [43m    [49m[43mcache_position[49m[43m=[49m[43mcache_position[49m[43m,[49m
[32m    267[39m [43m    [49m[43mposition_embeddings[49m[43m=[49m[43mposition_embeddings[49m[43m,[49m
[32m    268[39m [43m    [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
[32m    269[39m [43m[49m[43m)[49m
[32m    270[39m hidden_states = residual + hidden_states
[32m    272[39m [38;5;66;03m# Fully Connected[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/deprecation.py:172[39m, in [36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func[39m[34m(*args, **kwargs)[39m
[32m    168[39m [38;5;28;01melif[39;00m minimum_action [38;5;129;01min[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m is_torchdynamo_compiling():
[32m    169[39m     [38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead[39;00m
[32m    170[39m     warnings.warn(message, [38;5;167;01mFutureWarning[39;00m, stacklevel=[32m2[39m)
[32m--> [39m[32m172[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:200[39m, in [36mQwen3Attention.forward[39m[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)[39m
[32m    197[39m input_shape = hidden_states.shape[:-[32m1[39m]
[32m    198[39m hidden_shape = (*input_shape, -[32m1[39m, [38;5;28mself[39m.head_dim)
[32m--> [39m[32m200[39m query_states = [38;5;28mself[39m.q_norm([38;5;28;43mself[39;49m[43m.[49m[43mq_proj[49m[43m([49m[43mhidden_states[49m[43m)[49m.view(hidden_shape)).transpose([32m1[39m, [32m2[39m)
[32m    201[39m key_states = [38;5;28mself[39m.k_norm([38;5;28mself[39m.k_proj(hidden_states).view(hidden_shape)).transpose([32m1[39m, [32m2[39m)
[32m    202[39m value_states = [38;5;28mself[39m.v_proj(hidden_states).view(hidden_shape).transpose([32m1[39m, [32m2[39m)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1775[39m, in [36mModule._wrapped_call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1773[39m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m._compiled_call_impl(*args, **kwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[32m   1774[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1775[39m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[43m.[49m[43m_call_impl[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1786[39m, in [36mModule._call_impl[39m[34m(self, *args, **kwargs)[39m
[32m   1781[39m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[32m   1782[39m [38;5;66;03m# this function, and just call forward.[39;00m
[32m   1783[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m._backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m._forward_pre_hooks
[32m   1784[39m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[32m   1785[39m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[32m-> [39m[32m1786[39m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1788[39m result = [38;5;28;01mNone[39;00m
[32m   1789[39m called_always_called_hooks = [38;5;28mset[39m()

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/peft/tuners/lora/layer.py:807[39m, in [36mLinear.forward[39m[34m(self, x, *args, **kwargs)[39m
[32m    805[39m x = [38;5;28mself[39m._cast_input_dtype(x, lora_A.weight.dtype)
[32m    806[39m [38;5;28;01mif[39;00m active_adapter [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mself[39m.lora_variant:  [38;5;66;03m# vanilla LoRA[39;00m
[32m--> [39m[32m807[39m     result = result + [43mlora_B[49m[43m([49m[43mlora_A[49m[43m([49m[43mdropout[49m[43m([49m[43mx[49m[43m)[49m[43m)[49m[43m)[49m[43m [49m[43m*[49m[43m [49m[43mscaling[49m
[32m    808[39m [38;5;28;01melse[39;00m:
[32m    809[39m     result = [38;5;28mself[39m.lora_variant[active_adapter].forward(
[32m    810[39m         [38;5;28mself[39m,
[32m    811[39m         active_adapter=active_adapter,
[32m   (...)[39m[32m    815[39m         **kwargs,
[32m    816[39m     )

[31mOutOfMemoryError[39m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 1.69 MiB is free. Process 3975170 has 8.02 GiB memory in use. Process 3983462 has 8.11 GiB memory in use. Including non-PyTorch memory, this process has 28.23 GiB memory in use. Of the allocated memory 27.47 GiB is allocated by PyTorch, and 271.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

WARNING: 04_f2llm_4B_ft_text_clean.ipynb execution failed, continuing with next notebook...

==========================================
Starting execution of: 05_KaLM_ft_tweet.ipynb
==========================================
Error occurred. Notebook saved to: entregables/05_KaLM_ft_tweet_error.ipynb
Error: An error occurred while executing the following cell:
------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize(batch):
    encoding = tokenizer(batch[TEXT_COLUMN], truncation=True, padding="max_length", max_length=256)
    return encoding

train_dataset = Dataset.from_pandas(train_df[['id_EXIST', TEXT_COLUMN, 'label']]).map(tokenize, batched=True)
eval_dataset = Dataset.from_pandas(val_df[['id_EXIST', TEXT_COLUMN, 'label']]).map(tokenize, batched=True)
test_dataset = Dataset.from_pandas(test_df[['id_EXIST', TEXT_COLUMN, 'label']]).map(tokenize, batched=True)
------------------


[31m---------------------------------------------------------------------------[39m
[31mHTTPError[39m                                 Traceback (most recent call last)
[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:402[39m, in [36mhf_raise_for_status[39m[34m(response, endpoint_name)[39m
[32m    401[39m [38;5;28;01mtry[39;00m:
[32m--> [39m[32m402[39m     [43mresponse[49m[43m.[49m[43mraise_for_status[49m[43m([49m[43m)[49m
[32m    403[39m [38;5;28;01mexcept[39;00m HTTPError [38;5;28;01mas[39;00m e:

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/requests/models.py:1026[39m, in [36mResponse.raise_for_status[39m[34m(self)[39m
[32m   1025[39m [38;5;28;01mif[39;00m http_error_msg:
[32m-> [39m[32m1026[39m     [38;5;28;01mraise[39;00m HTTPError(http_error_msg, response=[38;5;28mself[39m)

[31mHTTPError[39m: 404 Client Error: Not Found for url: https://huggingface.co/FernandoLpz/KaLM-ft-cov19en-hs/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

[31mRepositoryNotFoundError[39m                   Traceback (most recent call last)
[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/hub.py:479[39m, in [36mcached_files[39m[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)[39m
[32m    477[39m [38;5;28;01mif[39;00m [38;5;28mlen[39m(full_filenames) == [32m1[39m:
[32m    478[39m     [38;5;66;03m# This is slightly better for only 1 file[39;00m
[32m--> [39m[32m479[39m     [43mhf_hub_download[49m[43m([49m
[32m    480[39m [43m        [49m[43mpath_or_repo_id[49m[43m,[49m
[32m    481[39m [43m        [49m[43mfilenames[49m[43m[[49m[32;43m0[39;49m[43m][49m[43m,[49m
[32m    482[39m [43m        [49m[43msubfolder[49m[43m=[49m[38;5;28;43;01mNone[39;49;00m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[38;5;28;43mlen[39;49m[43m([49m[43msubfolder[49m[43m)[49m[43m [49m[43m==[49m[43m [49m[32;43m0[39;49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[43msubfolder[49m[43m,[49m
[32m    483[39m [43m        [49m[43mrepo_type[49m[43m=[49m[43mrepo_type[49m[43m,[49m
[32m    484[39m [43m        [49m[43mrevision[49m[43m=[49m[43mrevision[49m[43m,[49m
[32m    485[39m [43m        [49m[43mcache_dir[49m[43m=[49m[43mcache_dir[49m[43m,[49m
[32m    486[39m [43m        [49m[43muser_agent[49m[43m=[49m[43muser_agent[49m[43m,[49m
[32m    487[39m [43m        [49m[43mforce_download[49m[43m=[49m[43mforce_download[49m[43m,[49m
[32m    488[39m [43m        [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m    489[39m [43m        [49m[43mresume_download[49m[43m=[49m[43mresume_download[49m[43m,[49m
[32m    490[39m [43m        [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m
[32m    491[39m [43m        [49m[43mlocal_files_only[49m[43m=[49m[43mlocal_files_only[49m[43m,[49m
[32m    492[39m [43m    [49m[43m)[49m
[32m    493[39m [38;5;28;01melse[39;00m:

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114[39m, in [36mvalidate_hf_hub_args.<locals>._inner_fn[39m[34m(*args, **kwargs)[39m
[32m    112[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.[34m__name__[39m, has_token=has_token, kwargs=kwargs)
[32m--> [39m[32m114[39m [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1007[39m, in [36mhf_hub_download[39m[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)[39m
[32m   1006[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1007[39m     [38;5;28;01mreturn[39;00m [43m_hf_hub_download_to_cache_dir[49m[43m([49m
[32m   1008[39m [43m        [49m[38;5;66;43;03m# Destination[39;49;00m
[32m   1009[39m [43m        [49m[43mcache_dir[49m[43m=[49m[43mcache_dir[49m[43m,[49m
[32m   1010[39m [43m        [49m[38;5;66;43;03m# File info[39;49;00m
[32m   1011[39m [43m        [49m[43mrepo_id[49m[43m=[49m[43mrepo_id[49m[43m,[49m
[32m   1012[39m [43m        [49m[43mfilename[49m[43m=[49m[43mfilename[49m[43m,[49m
[32m   1013[39m [43m        [49m[43mrepo_type[49m[43m=[49m[43mrepo_type[49m[43m,[49m
[32m   1014[39m [43m        [49m[43mrevision[49m[43m=[49m[43mrevision[49m[43m,[49m
[32m   1015[39m [43m        [49m[38;5;66;43;03m# HTTP info[39;49;00m
[32m   1016[39m [43m        [49m[43mendpoint[49m[43m=[49m[43mendpoint[49m[43m,[49m
[32m   1017[39m [43m        [49m[43metag_timeout[49m[43m=[49m[43metag_timeout[49m[43m,[49m
[32m   1018[39m [43m        [49m[43mheaders[49m[43m=[49m[43mhf_headers[49m[43m,[49m
[32m   1019[39m [43m        [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m   1020[39m [43m        [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m
[32m   1021[39m [43m        [49m[38;5;66;43;03m# Additional options[39;49;00m
[32m   1022[39m [43m        [49m[43mlocal_files_only[49m[43m=[49m[43mlocal_files_only[49m[43m,[49m
[32m   1023[39m [43m        [49m[43mforce_download[49m[43m=[49m[43mforce_download[49m[43m,[49m
[32m   1024[39m [43m    [49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1114[39m, in [36m_hf_hub_download_to_cache_dir[39m[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)[39m
[32m   1113[39m     [38;5;66;03m# Otherwise, raise appropriate error[39;00m
[32m-> [39m[32m1114[39m     [43m_raise_on_head_call_error[49m[43m([49m[43mhead_call_error[49m[43m,[49m[43m [49m[43mforce_download[49m[43m,[49m[43m [49m[43mlocal_files_only[49m[43m)[49m
[32m   1116[39m [38;5;66;03m# From now on, etag, commit_hash, url and size are not None.[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1655[39m, in [36m_raise_on_head_call_error[39m[34m(head_call_error, force_download, local_files_only)[39m
[32m   1650[39m [38;5;28;01melif[39;00m [38;5;28misinstance[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) [38;5;129;01mor[39;00m (
[32m   1651[39m     [38;5;28misinstance[39m(head_call_error, HfHubHTTPError) [38;5;129;01mand[39;00m head_call_error.response.status_code == [32m401[39m
[32m   1652[39m ):
[32m   1653[39m     [38;5;66;03m# Repo not found or gated => let's raise the actual error[39;00m
[32m   1654[39m     [38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error[39;00m
[32m-> [39m[32m1655[39m     [38;5;28;01mraise[39;00m head_call_error
[32m   1656[39m [38;5;28;01melse[39;00m:
[32m   1657[39m     [38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1543[39m, in [36m_get_metadata_or_catch_error[39m[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)[39m
[32m   1542[39m [38;5;28;01mtry[39;00m:
[32m-> [39m[32m1543[39m     metadata = [43mget_hf_file_metadata[49m[43m([49m
[32m   1544[39m [43m        [49m[43murl[49m[43m=[49m[43murl[49m[43m,[49m[43m [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m[43m [49m[43mtimeout[49m[43m=[49m[43metag_timeout[49m[43m,[49m[43m [49m[43mheaders[49m[43m=[49m[43mheaders[49m[43m,[49m[43m [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m[43m [49m[43mendpoint[49m[43m=[49m[43mendpoint[49m
[32m   1545[39m [43m    [49m[43m)[49m
[32m   1546[39m [38;5;28;01mexcept[39;00m EntryNotFoundError [38;5;28;01mas[39;00m http_error:

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114[39m, in [36mvalidate_hf_hub_args.<locals>._inner_fn[39m[34m(*args, **kwargs)[39m
[32m    112[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.[34m__name__[39m, has_token=has_token, kwargs=kwargs)
[32m--> [39m[32m114[39m [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1460[39m, in [36mget_hf_file_metadata[39m[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)[39m
[32m   1459[39m [38;5;66;03m# Retrieve metadata[39;00m
[32m-> [39m[32m1460[39m r = [43m_request_wrapper[49m[43m([49m
[32m   1461[39m [43m    [49m[43mmethod[49m[43m=[49m[33;43m"[39;49m[33;43mHEAD[39;49m[33;43m"[39;49m[43m,[49m
[32m   1462[39m [43m    [49m[43murl[49m[43m=[49m[43murl[49m[43m,[49m
[32m   1463[39m [43m    [49m[43mheaders[49m[43m=[49m[43mhf_headers[49m[43m,[49m
[32m   1464[39m [43m    [49m[43mallow_redirects[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m   1465[39m [43m    [49m[43mfollow_relative_redirects[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[32m   1466[39m [43m    [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m   1467[39m [43m    [49m[43mtimeout[49m[43m=[49m[43mtimeout[49m[43m,[49m
[32m   1468[39m [43m[49m[43m)[49m
[32m   1469[39m hf_raise_for_status(r)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:283[39m, in [36m_request_wrapper[39m[34m(method, url, follow_relative_redirects, **params)[39m
[32m    282[39m [38;5;28;01mif[39;00m follow_relative_redirects:
[32m--> [39m[32m283[39m     response = [43m_request_wrapper[49m[43m([49m
[32m    284[39m [43m        [49m[43mmethod[49m[43m=[49m[43mmethod[49m[43m,[49m
[32m    285[39m [43m        [49m[43murl[49m[43m=[49m[43murl[49m[43m,[49m
[32m    286[39m [43m        [49m[43mfollow_relative_redirects[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    287[39m [43m        [49m[43m*[49m[43m*[49m[43mparams[49m[43m,[49m
[32m    288[39m [43m    [49m[43m)[49m
[32m    290[39m     [38;5;66;03m# If redirection, we redirect only relative paths.[39;00m
[32m    291[39m     [38;5;66;03m# This is useful in case of a renamed repository.[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:307[39m, in [36m_request_wrapper[39m[34m(method, url, follow_relative_redirects, **params)[39m
[32m    306[39m response = http_backoff(method=method, url=url, **params)
[32m--> [39m[32m307[39m [43mhf_raise_for_status[49m[43m([49m[43mresponse[49m[43m)[49m
[32m    308[39m [38;5;28;01mreturn[39;00m response

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:452[39m, in [36mhf_raise_for_status[39m[34m(response, endpoint_name)[39m
[32m    443[39m     message = (
[32m    444[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mresponse.status_code[38;5;132;01m}[39;00m[33m Client Error.[39m[33m"[39m
[32m    445[39m         + [33m"[39m[38;5;130;01m\n[39;00m[38;5;130;01m\n[39;00m[33m"[39m
[32m   (...)[39m[32m    450[39m         [33m"[39m[33m https://huggingface.co/docs/huggingface_hub/authentication[39m[33m"[39m
[32m    451[39m     )
[32m--> [39m[32m452[39m     [38;5;28;01mraise[39;00m _format(RepositoryNotFoundError, message, response) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m
[32m    454[39m [38;5;28;01melif[39;00m response.status_code == [32m400[39m:

[31mRepositoryNotFoundError[39m: 404 Client Error. (Request ID: Root=1-69a036be-1ec4fb9b7e693135728d4581;c69fce33-3104-42d1-9924-8b2a4387d340)

Repository Not Found for url: https://huggingface.co/FernandoLpz/KaLM-ft-cov19en-hs/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

[31mOSError[39m                                   Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[4][39m[32m, line 1[39m
[32m----> [39m[32m1[39m tokenizer = [43mAutoTokenizer[49m[43m.[49m[43mfrom_pretrained[49m[43m([49m[43mMODEL_NAME[49m[43m)[49m
[32m      2[39m [38;5;28;01mif[39;00m tokenizer.pad_token [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[32m      3[39m     tokenizer.pad_token = tokenizer.eos_token

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1089[39m, in [36mAutoTokenizer.from_pretrained[39m[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)[39m
[32m   1086[39m     [38;5;28;01mreturn[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
[32m   1088[39m [38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.[39;00m
[32m-> [39m[32m1089[39m tokenizer_config = [43mget_tokenizer_config[49m[43m([49m[43mpretrained_model_name_or_path[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1090[39m [38;5;28;01mif[39;00m [33m"[39m[33m_commit_hash[39m[33m"[39m [38;5;129;01min[39;00m tokenizer_config:
[32m   1091[39m     kwargs[[33m"[39m[33m_commit_hash[39m[33m"[39m] = tokenizer_config[[33m"[39m[33m_commit_hash[39m[33m"[39m]

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:921[39m, in [36mget_tokenizer_config[39m[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)[39m
[32m    918[39m     token = use_auth_token
[32m    920[39m commit_hash = kwargs.get([33m"[39m[33m_commit_hash[39m[33m"[39m)
[32m--> [39m[32m921[39m resolved_config_file = [43mcached_file[49m[43m([49m
[32m    922[39m [43m    [49m[43mpretrained_model_name_or_path[49m[43m,[49m
[32m    923[39m [43m    [49m[43mTOKENIZER_CONFIG_FILE[49m[43m,[49m
[32m    924[39m [43m    [49m[43mcache_dir[49m[43m=[49m[43mcache_dir[49m[43m,[49m
[32m    925[39m [43m    [49m[43mforce_download[49m[43m=[49m[43mforce_download[49m[43m,[49m
[32m    926[39m [43m    [49m[43mresume_download[49m[43m=[49m[43mresume_download[49m[43m,[49m
[32m    927[39m [43m    [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m    928[39m [43m    [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m
[32m    929[39m [43m    [49m[43mrevision[49m[43m=[49m[43mrevision[49m[43m,[49m
[32m    930[39m [43m    [49m[43mlocal_files_only[49m[43m=[49m[43mlocal_files_only[49m[43m,[49m
[32m    931[39m [43m    [49m[43msubfolder[49m[43m=[49m[43msubfolder[49m[43m,[49m
[32m    932[39m [43m    [49m[43m_raise_exceptions_for_gated_repo[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    933[39m [43m    [49m[43m_raise_exceptions_for_missing_entries[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    934[39m [43m    [49m[43m_raise_exceptions_for_connection_errors[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    935[39m [43m    [49m[43m_commit_hash[49m[43m=[49m[43mcommit_hash[49m[43m,[49m
[32m    936[39m [43m[49m[43m)[49m
[32m    937[39m [38;5;28;01mif[39;00m resolved_config_file [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[32m    938[39m     logger.info([33m"[39m[33mCould not locate the tokenizer configuration file, will try to use the model config instead.[39m[33m"[39m)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/hub.py:322[39m, in [36mcached_file[39m[34m(path_or_repo_id, filename, **kwargs)[39m
[32m    264[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mcached_file[39m(
[32m    265[39m     path_or_repo_id: Union[[38;5;28mstr[39m, os.PathLike],
[32m    266[39m     filename: [38;5;28mstr[39m,
[32m    267[39m     **kwargs,
[32m    268[39m ) -> Optional[[38;5;28mstr[39m]:
[32m    269[39m [38;5;250m    [39m[33;03m"""[39;00m
[32m    270[39m [33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.[39;00m
[32m    271[39m 
[32m   (...)[39m[32m    320[39m [33;03m    ```[39;00m
[32m    321[39m [33;03m    """[39;00m
[32m--> [39m[32m322[39m     file = [43mcached_files[49m[43m([49m[43mpath_or_repo_id[49m[43m=[49m[43mpath_or_repo_id[49m[43m,[49m[43m [49m[43mfilenames[49m[43m=[49m[43m[[49m[43mfilename[49m[43m][49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m    323[39m     file = file[[32m0[39m] [38;5;28;01mif[39;00m file [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;28;01melse[39;00m file
[32m    324[39m     [38;5;28;01mreturn[39;00m file

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/hub.py:511[39m, in [36mcached_files[39m[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)[39m
[32m    508[39m [38;5;28;01mexcept[39;00m [38;5;167;01mException[39;00m [38;5;28;01mas[39;00m e:
[32m    509[39m     [38;5;66;03m# We cannot recover from them[39;00m
[32m    510[39m     [38;5;28;01mif[39;00m [38;5;28misinstance[39m(e, RepositoryNotFoundError) [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28misinstance[39m(e, GatedRepoError):
[32m--> [39m[32m511[39m         [38;5;28;01mraise[39;00m [38;5;167;01mOSError[39;00m(
[32m    512[39m             [33mf[39m[33m"[39m[38;5;132;01m{[39;00mpath_or_repo_id[38;5;132;01m}[39;00m[33m is not a local folder and is not a valid model identifier [39m[33m"[39m
[32m    513[39m             [33m"[39m[33mlisted on [39m[33m'[39m[33mhttps://huggingface.co/models[39m[33m'[39m[38;5;130;01m\n[39;00m[33mIf this is a private repository, make sure to pass a token [39m[33m"[39m
[32m    514[39m             [33m"[39m[33mhaving permission to this repo either by logging in with `hf auth login` or by passing [39m[33m"[39m
[32m    515[39m             [33m"[39m[33m`token=<your_token>`[39m[33m"[39m
[32m    516[39m         ) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m
[32m    517[39m     [38;5;28;01melif[39;00m [38;5;28misinstance[39m(e, RevisionNotFoundError):
[32m    518[39m         [38;5;28;01mraise[39;00m [38;5;167;01mOSError[39;00m(
[32m    519[39m             [33mf[39m[33m"[39m[38;5;132;01m{[39;00mrevision[38;5;132;01m}[39;00m[33m is not a valid git identifier (branch name, tag name or commit id) that exists [39m[33m"[39m
[32m    520[39m             [33m"[39m[33mfor this model name. Check the model page at [39m[33m"[39m
[32m    521[39m             [33mf[39m[33m"[39m[33m'[39m[33mhttps://huggingface.co/[39m[38;5;132;01m{[39;00mpath_or_repo_id[38;5;132;01m}[39;00m[33m'[39m[33m for available revisions.[39m[33m"[39m
[32m    522[39m         ) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m

[31mOSError[39m: FernandoLpz/KaLM-ft-cov19en-hs is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`

WARNING: 05_KaLM_ft_tweet.ipynb execution failed, continuing with next notebook...

==========================================
Starting execution of: 06_KaLM_ft_text_clean.ipynb
==========================================
Error occurred. Notebook saved to: entregables/06_KaLM_ft_text_clean_error.ipynb
Error: An error occurred while executing the following cell:
------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize(batch):
    encoding = tokenizer(batch[TEXT_COLUMN], truncation=True, padding="max_length", max_length=256)
    return encoding

train_dataset = Dataset.from_pandas(train_df[['id_EXIST', TEXT_COLUMN, 'label']]).map(tokenize, batched=True)
eval_dataset = Dataset.from_pandas(val_df[['id_EXIST', TEXT_COLUMN, 'label']]).map(tokenize, batched=True)
test_dataset = Dataset.from_pandas(test_df[['id_EXIST', TEXT_COLUMN, 'label']]).map(tokenize, batched=True)
------------------


[31m---------------------------------------------------------------------------[39m
[31mHTTPError[39m                                 Traceback (most recent call last)
[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:402[39m, in [36mhf_raise_for_status[39m[34m(response, endpoint_name)[39m
[32m    401[39m [38;5;28;01mtry[39;00m:
[32m--> [39m[32m402[39m     [43mresponse[49m[43m.[49m[43mraise_for_status[49m[43m([49m[43m)[49m
[32m    403[39m [38;5;28;01mexcept[39;00m HTTPError [38;5;28;01mas[39;00m e:

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/requests/models.py:1026[39m, in [36mResponse.raise_for_status[39m[34m(self)[39m
[32m   1025[39m [38;5;28;01mif[39;00m http_error_msg:
[32m-> [39m[32m1026[39m     [38;5;28;01mraise[39;00m HTTPError(http_error_msg, response=[38;5;28mself[39m)

[31mHTTPError[39m: 404 Client Error: Not Found for url: https://huggingface.co/FernandoLpz/KaLM-ft-cov19en-hs/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

[31mRepositoryNotFoundError[39m                   Traceback (most recent call last)
[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/hub.py:479[39m, in [36mcached_files[39m[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)[39m
[32m    477[39m [38;5;28;01mif[39;00m [38;5;28mlen[39m(full_filenames) == [32m1[39m:
[32m    478[39m     [38;5;66;03m# This is slightly better for only 1 file[39;00m
[32m--> [39m[32m479[39m     [43mhf_hub_download[49m[43m([49m
[32m    480[39m [43m        [49m[43mpath_or_repo_id[49m[43m,[49m
[32m    481[39m [43m        [49m[43mfilenames[49m[43m[[49m[32;43m0[39;49m[43m][49m[43m,[49m
[32m    482[39m [43m        [49m[43msubfolder[49m[43m=[49m[38;5;28;43;01mNone[39;49;00m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[38;5;28;43mlen[39;49m[43m([49m[43msubfolder[49m[43m)[49m[43m [49m[43m==[49m[43m [49m[32;43m0[39;49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[43msubfolder[49m[43m,[49m
[32m    483[39m [43m        [49m[43mrepo_type[49m[43m=[49m[43mrepo_type[49m[43m,[49m
[32m    484[39m [43m        [49m[43mrevision[49m[43m=[49m[43mrevision[49m[43m,[49m
[32m    485[39m [43m        [49m[43mcache_dir[49m[43m=[49m[43mcache_dir[49m[43m,[49m
[32m    486[39m [43m        [49m[43muser_agent[49m[43m=[49m[43muser_agent[49m[43m,[49m
[32m    487[39m [43m        [49m[43mforce_download[49m[43m=[49m[43mforce_download[49m[43m,[49m
[32m    488[39m [43m        [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m    489[39m [43m        [49m[43mresume_download[49m[43m=[49m[43mresume_download[49m[43m,[49m
[32m    490[39m [43m        [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m
[32m    491[39m [43m        [49m[43mlocal_files_only[49m[43m=[49m[43mlocal_files_only[49m[43m,[49m
[32m    492[39m [43m    [49m[43m)[49m
[32m    493[39m [38;5;28;01melse[39;00m:

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114[39m, in [36mvalidate_hf_hub_args.<locals>._inner_fn[39m[34m(*args, **kwargs)[39m
[32m    112[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.[34m__name__[39m, has_token=has_token, kwargs=kwargs)
[32m--> [39m[32m114[39m [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1007[39m, in [36mhf_hub_download[39m[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)[39m
[32m   1006[39m [38;5;28;01melse[39;00m:
[32m-> [39m[32m1007[39m     [38;5;28;01mreturn[39;00m [43m_hf_hub_download_to_cache_dir[49m[43m([49m
[32m   1008[39m [43m        [49m[38;5;66;43;03m# Destination[39;49;00m
[32m   1009[39m [43m        [49m[43mcache_dir[49m[43m=[49m[43mcache_dir[49m[43m,[49m
[32m   1010[39m [43m        [49m[38;5;66;43;03m# File info[39;49;00m
[32m   1011[39m [43m        [49m[43mrepo_id[49m[43m=[49m[43mrepo_id[49m[43m,[49m
[32m   1012[39m [43m        [49m[43mfilename[49m[43m=[49m[43mfilename[49m[43m,[49m
[32m   1013[39m [43m        [49m[43mrepo_type[49m[43m=[49m[43mrepo_type[49m[43m,[49m
[32m   1014[39m [43m        [49m[43mrevision[49m[43m=[49m[43mrevision[49m[43m,[49m
[32m   1015[39m [43m        [49m[38;5;66;43;03m# HTTP info[39;49;00m
[32m   1016[39m [43m        [49m[43mendpoint[49m[43m=[49m[43mendpoint[49m[43m,[49m
[32m   1017[39m [43m        [49m[43metag_timeout[49m[43m=[49m[43metag_timeout[49m[43m,[49m
[32m   1018[39m [43m        [49m[43mheaders[49m[43m=[49m[43mhf_headers[49m[43m,[49m
[32m   1019[39m [43m        [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m   1020[39m [43m        [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m
[32m   1021[39m [43m        [49m[38;5;66;43;03m# Additional options[39;49;00m
[32m   1022[39m [43m        [49m[43mlocal_files_only[49m[43m=[49m[43mlocal_files_only[49m[43m,[49m
[32m   1023[39m [43m        [49m[43mforce_download[49m[43m=[49m[43mforce_download[49m[43m,[49m
[32m   1024[39m [43m    [49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1114[39m, in [36m_hf_hub_download_to_cache_dir[39m[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)[39m
[32m   1113[39m     [38;5;66;03m# Otherwise, raise appropriate error[39;00m
[32m-> [39m[32m1114[39m     [43m_raise_on_head_call_error[49m[43m([49m[43mhead_call_error[49m[43m,[49m[43m [49m[43mforce_download[49m[43m,[49m[43m [49m[43mlocal_files_only[49m[43m)[49m
[32m   1116[39m [38;5;66;03m# From now on, etag, commit_hash, url and size are not None.[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1655[39m, in [36m_raise_on_head_call_error[39m[34m(head_call_error, force_download, local_files_only)[39m
[32m   1650[39m [38;5;28;01melif[39;00m [38;5;28misinstance[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) [38;5;129;01mor[39;00m (
[32m   1651[39m     [38;5;28misinstance[39m(head_call_error, HfHubHTTPError) [38;5;129;01mand[39;00m head_call_error.response.status_code == [32m401[39m
[32m   1652[39m ):
[32m   1653[39m     [38;5;66;03m# Repo not found or gated => let's raise the actual error[39;00m
[32m   1654[39m     [38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error[39;00m
[32m-> [39m[32m1655[39m     [38;5;28;01mraise[39;00m head_call_error
[32m   1656[39m [38;5;28;01melse[39;00m:
[32m   1657[39m     [38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1543[39m, in [36m_get_metadata_or_catch_error[39m[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)[39m
[32m   1542[39m [38;5;28;01mtry[39;00m:
[32m-> [39m[32m1543[39m     metadata = [43mget_hf_file_metadata[49m[43m([49m
[32m   1544[39m [43m        [49m[43murl[49m[43m=[49m[43murl[49m[43m,[49m[43m [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m[43m [49m[43mtimeout[49m[43m=[49m[43metag_timeout[49m[43m,[49m[43m [49m[43mheaders[49m[43m=[49m[43mheaders[49m[43m,[49m[43m [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m[43m [49m[43mendpoint[49m[43m=[49m[43mendpoint[49m
[32m   1545[39m [43m    [49m[43m)[49m
[32m   1546[39m [38;5;28;01mexcept[39;00m EntryNotFoundError [38;5;28;01mas[39;00m http_error:

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114[39m, in [36mvalidate_hf_hub_args.<locals>._inner_fn[39m[34m(*args, **kwargs)[39m
[32m    112[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.[34m__name__[39m, has_token=has_token, kwargs=kwargs)
[32m--> [39m[32m114[39m [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1460[39m, in [36mget_hf_file_metadata[39m[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)[39m
[32m   1459[39m [38;5;66;03m# Retrieve metadata[39;00m
[32m-> [39m[32m1460[39m r = [43m_request_wrapper[49m[43m([49m
[32m   1461[39m [43m    [49m[43mmethod[49m[43m=[49m[33;43m"[39;49m[33;43mHEAD[39;49m[33;43m"[39;49m[43m,[49m
[32m   1462[39m [43m    [49m[43murl[49m[43m=[49m[43murl[49m[43m,[49m
[32m   1463[39m [43m    [49m[43mheaders[49m[43m=[49m[43mhf_headers[49m[43m,[49m
[32m   1464[39m [43m    [49m[43mallow_redirects[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m   1465[39m [43m    [49m[43mfollow_relative_redirects[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[32m   1466[39m [43m    [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m   1467[39m [43m    [49m[43mtimeout[49m[43m=[49m[43mtimeout[49m[43m,[49m
[32m   1468[39m [43m[49m[43m)[49m
[32m   1469[39m hf_raise_for_status(r)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:283[39m, in [36m_request_wrapper[39m[34m(method, url, follow_relative_redirects, **params)[39m
[32m    282[39m [38;5;28;01mif[39;00m follow_relative_redirects:
[32m--> [39m[32m283[39m     response = [43m_request_wrapper[49m[43m([49m
[32m    284[39m [43m        [49m[43mmethod[49m[43m=[49m[43mmethod[49m[43m,[49m
[32m    285[39m [43m        [49m[43murl[49m[43m=[49m[43murl[49m[43m,[49m
[32m    286[39m [43m        [49m[43mfollow_relative_redirects[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    287[39m [43m        [49m[43m*[49m[43m*[49m[43mparams[49m[43m,[49m
[32m    288[39m [43m    [49m[43m)[49m
[32m    290[39m     [38;5;66;03m# If redirection, we redirect only relative paths.[39;00m
[32m    291[39m     [38;5;66;03m# This is useful in case of a renamed repository.[39;00m

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/file_download.py:307[39m, in [36m_request_wrapper[39m[34m(method, url, follow_relative_redirects, **params)[39m
[32m    306[39m response = http_backoff(method=method, url=url, **params)
[32m--> [39m[32m307[39m [43mhf_raise_for_status[49m[43m([49m[43mresponse[49m[43m)[49m
[32m    308[39m [38;5;28;01mreturn[39;00m response

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:452[39m, in [36mhf_raise_for_status[39m[34m(response, endpoint_name)[39m
[32m    443[39m     message = (
[32m    444[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mresponse.status_code[38;5;132;01m}[39;00m[33m Client Error.[39m[33m"[39m
[32m    445[39m         + [33m"[39m[38;5;130;01m\n[39;00m[38;5;130;01m\n[39;00m[33m"[39m
[32m   (...)[39m[32m    450[39m         [33m"[39m[33m https://huggingface.co/docs/huggingface_hub/authentication[39m[33m"[39m
[32m    451[39m     )
[32m--> [39m[32m452[39m     [38;5;28;01mraise[39;00m _format(RepositoryNotFoundError, message, response) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m
[32m    454[39m [38;5;28;01melif[39;00m response.status_code == [32m400[39m:

[31mRepositoryNotFoundError[39m: 404 Client Error. (Request ID: Root=1-69a036c7-0ecceed50a8a27bf494c5241;19d1a345-bd9f-4793-aef0-3de0f9dc8e08)

Repository Not Found for url: https://huggingface.co/FernandoLpz/KaLM-ft-cov19en-hs/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:

[31mOSError[39m                                   Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[4][39m[32m, line 1[39m
[32m----> [39m[32m1[39m tokenizer = [43mAutoTokenizer[49m[43m.[49m[43mfrom_pretrained[49m[43m([49m[43mMODEL_NAME[49m[43m)[49m
[32m      2[39m [38;5;28;01mif[39;00m tokenizer.pad_token [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[32m      3[39m     tokenizer.pad_token = tokenizer.eos_token

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1089[39m, in [36mAutoTokenizer.from_pretrained[39m[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)[39m
[32m   1086[39m     [38;5;28;01mreturn[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
[32m   1088[39m [38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.[39;00m
[32m-> [39m[32m1089[39m tokenizer_config = [43mget_tokenizer_config[49m[43m([49m[43mpretrained_model_name_or_path[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m   1090[39m [38;5;28;01mif[39;00m [33m"[39m[33m_commit_hash[39m[33m"[39m [38;5;129;01min[39;00m tokenizer_config:
[32m   1091[39m     kwargs[[33m"[39m[33m_commit_hash[39m[33m"[39m] = tokenizer_config[[33m"[39m[33m_commit_hash[39m[33m"[39m]

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:921[39m, in [36mget_tokenizer_config[39m[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)[39m
[32m    918[39m     token = use_auth_token
[32m    920[39m commit_hash = kwargs.get([33m"[39m[33m_commit_hash[39m[33m"[39m)
[32m--> [39m[32m921[39m resolved_config_file = [43mcached_file[49m[43m([49m
[32m    922[39m [43m    [49m[43mpretrained_model_name_or_path[49m[43m,[49m
[32m    923[39m [43m    [49m[43mTOKENIZER_CONFIG_FILE[49m[43m,[49m
[32m    924[39m [43m    [49m[43mcache_dir[49m[43m=[49m[43mcache_dir[49m[43m,[49m
[32m    925[39m [43m    [49m[43mforce_download[49m[43m=[49m[43mforce_download[49m[43m,[49m
[32m    926[39m [43m    [49m[43mresume_download[49m[43m=[49m[43mresume_download[49m[43m,[49m
[32m    927[39m [43m    [49m[43mproxies[49m[43m=[49m[43mproxies[49m[43m,[49m
[32m    928[39m [43m    [49m[43mtoken[49m[43m=[49m[43mtoken[49m[43m,[49m
[32m    929[39m [43m    [49m[43mrevision[49m[43m=[49m[43mrevision[49m[43m,[49m
[32m    930[39m [43m    [49m[43mlocal_files_only[49m[43m=[49m[43mlocal_files_only[49m[43m,[49m
[32m    931[39m [43m    [49m[43msubfolder[49m[43m=[49m[43msubfolder[49m[43m,[49m
[32m    932[39m [43m    [49m[43m_raise_exceptions_for_gated_repo[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    933[39m [43m    [49m[43m_raise_exceptions_for_missing_entries[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    934[39m [43m    [49m[43m_raise_exceptions_for_connection_errors[49m[43m=[49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[32m    935[39m [43m    [49m[43m_commit_hash[49m[43m=[49m[43mcommit_hash[49m[43m,[49m
[32m    936[39m [43m[49m[43m)[49m
[32m    937[39m [38;5;28;01mif[39;00m resolved_config_file [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[32m    938[39m     logger.info([33m"[39m[33mCould not locate the tokenizer configuration file, will try to use the model config instead.[39m[33m"[39m)

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/hub.py:322[39m, in [36mcached_file[39m[34m(path_or_repo_id, filename, **kwargs)[39m
[32m    264[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mcached_file[39m(
[32m    265[39m     path_or_repo_id: Union[[38;5;28mstr[39m, os.PathLike],
[32m    266[39m     filename: [38;5;28mstr[39m,
[32m    267[39m     **kwargs,
[32m    268[39m ) -> Optional[[38;5;28mstr[39m]:
[32m    269[39m [38;5;250m    [39m[33;03m"""[39;00m
[32m    270[39m [33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.[39;00m
[32m    271[39m 
[32m   (...)[39m[32m    320[39m [33;03m    ```[39;00m
[32m    321[39m [33;03m    """[39;00m
[32m--> [39m[32m322[39m     file = [43mcached_files[49m[43m([49m[43mpath_or_repo_id[49m[43m=[49m[43mpath_or_repo_id[49m[43m,[49m[43m [49m[43mfilenames[49m[43m=[49m[43m[[49m[43mfilename[49m[43m][49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m    323[39m     file = file[[32m0[39m] [38;5;28;01mif[39;00m file [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;28;01melse[39;00m file
[32m    324[39m     [38;5;28;01mreturn[39;00m file

[36mFile [39m[32m~/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/utils/hub.py:511[39m, in [36mcached_files[39m[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)[39m
[32m    508[39m [38;5;28;01mexcept[39;00m [38;5;167;01mException[39;00m [38;5;28;01mas[39;00m e:
[32m    509[39m     [38;5;66;03m# We cannot recover from them[39;00m
[32m    510[39m     [38;5;28;01mif[39;00m [38;5;28misinstance[39m(e, RepositoryNotFoundError) [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28misinstance[39m(e, GatedRepoError):
[32m--> [39m[32m511[39m         [38;5;28;01mraise[39;00m [38;5;167;01mOSError[39;00m(
[32m    512[39m             [33mf[39m[33m"[39m[38;5;132;01m{[39;00mpath_or_repo_id[38;5;132;01m}[39;00m[33m is not a local folder and is not a valid model identifier [39m[33m"[39m
[32m    513[39m             [33m"[39m[33mlisted on [39m[33m'[39m[33mhttps://huggingface.co/models[39m[33m'[39m[38;5;130;01m\n[39;00m[33mIf this is a private repository, make sure to pass a token [39m[33m"[39m
[32m    514[39m             [33m"[39m[33mhaving permission to this repo either by logging in with `hf auth login` or by passing [39m[33m"[39m
[32m    515[39m             [33m"[39m[33m`token=<your_token>`[39m[33m"[39m
[32m    516[39m         ) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m
[32m    517[39m     [38;5;28;01melif[39;00m [38;5;28misinstance[39m(e, RevisionNotFoundError):
[32m    518[39m         [38;5;28;01mraise[39;00m [38;5;167;01mOSError[39;00m(
[32m    519[39m             [33mf[39m[33m"[39m[38;5;132;01m{[39;00mrevision[38;5;132;01m}[39;00m[33m is not a valid git identifier (branch name, tag name or commit id) that exists [39m[33m"[39m
[32m    520[39m             [33m"[39m[33mfor this model name. Check the model page at [39m[33m"[39m
[32m    521[39m             [33mf[39m[33m"[39m[33m'[39m[33mhttps://huggingface.co/[39m[38;5;132;01m{[39;00mpath_or_repo_id[38;5;132;01m}[39;00m[33m'[39m[33m for available revisions.[39m[33m"[39m
[32m    522[39m         ) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m

[31mOSError[39m: FernandoLpz/KaLM-ft-cov19en-hs is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`

WARNING: 06_KaLM_ft_text_clean.ipynb execution failed, continuing with next notebook...

==========================================
Starting execution of: 07_Ministral3_8B_inference_tweet.ipynb
==========================================
Error occurred. Notebook saved to: entregables/07_Ministral3_8B_inference_tweet_error.ipynb
Error: An error occurred while executing the following cell:
------------------
import os
import torch
import pandas as pd
import numpy as np

from transformers import MistralCommonBackend, Mistral3ForConditionalGeneration, FineGrainedFP8Config
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support
)

from pyevall.evaluation import PyEvALLEvaluation
from pyevall.metrics.metricfactory import MetricFactory
------------------


[31m---------------------------------------------------------------------------[39m
[31mImportError[39m                               Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[1][39m[32m, line 6[39m
[32m      3[39m [38;5;28;01mimport[39;00m[38;5;250m [39m[34;01mpandas[39;00m[38;5;250m [39m[38;5;28;01mas[39;00m[38;5;250m [39m[34;01mpd[39;00m
[32m      4[39m [38;5;28;01mimport[39;00m[38;5;250m [39m[34;01mnumpy[39;00m[38;5;250m [39m[38;5;28;01mas[39;00m[38;5;250m [39m[34;01mnp[39;00m
[32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mtransformers[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m MistralCommonBackend, Mistral3ForConditionalGeneration, FineGrainedFP8Config
[32m      7[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01msklearn[39;00m[34;01m.[39;00m[34;01mmetrics[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m (
[32m      8[39m     accuracy_score,
[32m      9[39m     precision_recall_fscore_support
[32m     10[39m )
[32m     12[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mpyevall[39;00m[34;01m.[39;00m[34;01mevaluation[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m PyEvALLEvaluation

[31mImportError[39m: cannot import name 'MistralCommonBackend' from 'transformers' (/home/alumno.upv.es/scheng1/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/__init__.py)

WARNING: 07_Ministral3_8B_inference_tweet.ipynb execution failed, continuing with next notebook...

==========================================
Starting execution of: 08_Ministral3_8B_only_ft.ipynb
==========================================
Error occurred. Notebook saved to: entregables/08_Ministral3_8B_only_ft_error.ipynb
Error: An error occurred while executing the following cell:
------------------
import os
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader

from transformers import (
    Mistral3ForConditionalGeneration,
    FineGrainedFP8Config,
    MistralCommonBackend,
    get_cosine_schedule_with_warmup,
)
from peft import LoraConfig, get_peft_model, TaskType
------------------


[31m---------------------------------------------------------------------------[39m
[31mImportError[39m                               Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[1][39m[32m, line 8[39m
[32m      5[39m [38;5;28;01mimport[39;00m[38;5;250m [39m[34;01mmatplotlib[39;00m[34;01m.[39;00m[34;01mpyplot[39;00m[38;5;250m [39m[38;5;28;01mas[39;00m[38;5;250m [39m[34;01mplt[39;00m
[32m      6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mtorch[39;00m[34;01m.[39;00m[34;01mutils[39;00m[34;01m.[39;00m[34;01mdata[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m Dataset, DataLoader
[32m----> [39m[32m8[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mtransformers[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m (
[32m      9[39m     Mistral3ForConditionalGeneration,
[32m     10[39m     FineGrainedFP8Config,
[32m     11[39m     MistralCommonBackend,
[32m     12[39m     get_cosine_schedule_with_warmup,
[32m     13[39m )
[32m     14[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mpeft[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m LoraConfig, get_peft_model, TaskType

[31mImportError[39m: cannot import name 'MistralCommonBackend' from 'transformers' (/home/alumno.upv.es/scheng1/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/__init__.py)

WARNING: 08_Ministral3_8B_only_ft.ipynb execution failed, continuing with next notebook...

==========================================
Starting execution of: 09_Ministral3_8B_inference_ft.ipynb
==========================================
Error occurred. Notebook saved to: entregables/09_Ministral3_8B_inference_ft_error.ipynb
Error: An error occurred while executing the following cell:
------------------
import os
import torch
import pandas as pd
import numpy as np

from transformers import Mistral3ForConditionalGeneration, FineGrainedFP8Config, MistralCommonBackend
from peft import PeftModel

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from pyevall.evaluation import PyEvALLEvaluation
from pyevall.metrics.metricfactory import MetricFactory
------------------


[31m---------------------------------------------------------------------------[39m
[31mImportError[39m                               Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[1][39m[32m, line 6[39m
[32m      3[39m [38;5;28;01mimport[39;00m[38;5;250m [39m[34;01mpandas[39;00m[38;5;250m [39m[38;5;28;01mas[39;00m[38;5;250m [39m[34;01mpd[39;00m
[32m      4[39m [38;5;28;01mimport[39;00m[38;5;250m [39m[34;01mnumpy[39;00m[38;5;250m [39m[38;5;28;01mas[39;00m[38;5;250m [39m[34;01mnp[39;00m
[32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mtransformers[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m Mistral3ForConditionalGeneration, FineGrainedFP8Config, MistralCommonBackend
[32m      7[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mpeft[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m PeftModel
[32m      9[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01msklearn[39;00m[34;01m.[39;00m[34;01mmetrics[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m accuracy_score, precision_recall_fscore_support

[31mImportError[39m: cannot import name 'MistralCommonBackend' from 'transformers' (/home/alumno.upv.es/scheng1/.conda/envs/RFA2526pt/lib/python3.12/site-packages/transformers/__init__.py)

WARNING: 09_Ministral3_8B_inference_ft.ipynb execution failed, continuing with next notebook...

==========================================
All notebooks processed!
==========================================
